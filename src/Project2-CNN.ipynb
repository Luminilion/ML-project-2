{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generic Functions\n",
    "Here we define all the functions to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, pos_file, neg_file, size=1_250_000): \n",
    "\n",
    "    # positive\n",
    "    pos = pd.read_table(data_path+pos_file, sep='.\\n', names=['tweet'], engine='python')\n",
    "    pos['label']=1\n",
    "    print(f\"Loaded POS data, correctly interpreted 1-tweet-per-line fashion : {pos.shape[0]==size}\")\n",
    "\n",
    "    # negative\n",
    "    neg = pd.read_table(data_path+neg_file, sep='.\\n', names=['tweet'], engine='python')\n",
    "    neg['label']=-1\n",
    "    print(f\"Loaded NEG data, correctly interpreted 1-tweet-per-line fashion : {neg.shape[0]==size}\")\n",
    "\n",
    "    # Data sizes\n",
    "    print(f\"Number of tweets : (POS) {pos.shape[0]} (NEG) {neg.shape[0]}\\n\")\n",
    "\n",
    "    # Merge datasets to get a complete training set\n",
    "    tweets = pos.append(neg)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove_helper import concatenate\n",
    "\n",
    "def load_embeddings_and_vocab(embeddings_loc, embeddings_path, vocab_loc, divided=False) :\n",
    "    \n",
    "    ## Load word embeddings and vocabulary to compute word vectors of tweets -----------------------------------\n",
    "    \n",
    "    # Load word embeddings\n",
    "    embeddings=None\n",
    "    if divided : \n",
    "        embeddings = concatenate(embeddings_files, embeddings_path)\n",
    "    else :\n",
    "        embeddings = np.load(embeddings_path+embeddings_loc)\n",
    "    print(f'Loaded word embeddings in structure of type {type(embeddings)}.')\n",
    "\n",
    "    # Loading vocab\n",
    "    words = pd.read_table(vocab_loc, sep='.\\n', names=['word'], engine='python', squeeze=True, na_values=np.nan)\n",
    "    print(f'Loaded word vocabulary in structure of type {type(words)}.')\n",
    "\n",
    "    # Check that the vocabulary encompasses all embedded words\n",
    "    print(f'\\nBoth the embeddings and the vocabulary are same length :  {len(embeddings)==words.shape[0]}')\n",
    "    print(f\"Embeddings: {embeddings.shape}, vocab: {words.shape}\")\n",
    "\n",
    "    ## Clean the data --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Drop NaN values in vocab\n",
    "    nas = words.isna()\n",
    "    words.dropna(inplace=True)\n",
    "    # Drop NaN words in embeddings\n",
    "    embeddings = np.delete(embeddings, nas[nas].index.values, axis=0)\n",
    "    \n",
    "    print(f'NaN values were dropped in both tables: {len(embeddings)==words.shape[0]}')\n",
    "    print(f\"Embeddings: {embeddings.shape}, vocab: {words.shape}\")\n",
    "\n",
    "    ## Process data ---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Index by words for faster index-for-word search\n",
    "    words = pd.DataFrame(data=words.index, index=words.values)\n",
    "    embeddings = pd.DataFrame(embeddings, index=words.index)\n",
    "    \n",
    "    return embeddings, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def resample_data(data, size, label=1, seed=None) :\n",
    "    # Get pos and neg datasets\n",
    "    pos, neg = data.loc[data['label']==label], data.loc[data['label']!=label]\n",
    "    \n",
    "    # Get samples balanced by classes\n",
    "    n = int(size/2)\n",
    "    pos_ = resample(pos, n_samples=n, replace=False, random_state=seed)\n",
    "    neg_ = resample(neg, n_samples=n, replace=False, random_state=seed)\n",
    "    \n",
    "    # reform dataset\n",
    "    data_ = pos_.append(neg_)\n",
    "\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tweet, embeddings, embeddings_dim, vocab, agg_func=None):\n",
    "    \"\"\"\n",
    "    Creates the feature vector corresponding to the tweet.\n",
    "    To do so, computes the mean of the word embeddings corresponding to the vocabulary words in the tweet.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    tweet : str\n",
    "        Input tweet from which the word vector is created.\n",
    "    \"\"\"\n",
    "    split_by_words = tweet.split()\n",
    "    embed_list = []\n",
    "    \n",
    "    # Get words in tweet that are in vocab\n",
    "    # Get embeddings for these words\n",
    "    for w in split_by_words:\n",
    "        if w in vocab.index :\n",
    "            embed_list.append(  embeddings.loc[w].values  )\n",
    "            \n",
    "    if agg_func is None :\n",
    "        agg_func = lambda x : np.mean(x, axis=0)\n",
    "    \n",
    "    result = None\n",
    "    # If no vocab word create an empty vector for the tweet\n",
    "    if not embed_list :\n",
    "        result = np.zeros(embeddings_dim)\n",
    "        \n",
    "    # Else aggregate the word embeddings\n",
    "    else :\n",
    "        result = agg_func(embed_list)\n",
    "    \n",
    "    # Be sure to output a list to store in dataframe\n",
    "    if type(result) is not list:\n",
    "        result = result.tolist()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove_helper import concatenate\n",
    "\n",
    "def load_word_vectors(file_path, file_loc, divided=False): \n",
    "    # Load pre-computed word vectors file\n",
    "    precomputed = None\n",
    "    if divided :\n",
    "        print('Loading from divided dataset...')\n",
    "        precomputed = concatenate(word_vectors_files, file_path)\n",
    "    else : \n",
    "        precomputed = np.load(file_path+file_loc, allow_pickle=True)\n",
    "        \n",
    "    precomputed = pd.DataFrame(precomputed, columns=['index', 'label', 'mean_embed'])\n",
    "    print('Successfully loaded from file!')\n",
    "    \n",
    "    return precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project2_helper import isAdmin, parallelize\n",
    "\n",
    "def compute_word_vectors(data, embeddings, embeddings_dim, vocab, agg_func=None) :\n",
    "    data = data.copy()\n",
    "    \n",
    "    # compute function to run on tweets\n",
    "    func = lambda t : word_vector(t, embeddings, embeddings_dim, vocab, agg_func=agg_func)\n",
    "    \n",
    "    if isAdmin():\n",
    "        print('Process is run as admin. Running parallelized computation...')\n",
    "        data['mean_embed']= parallelize(data['tweet'], func)\n",
    "    else : \n",
    "        print('Process is not run as admin. Cannot run parallelized setting, running as sequential...')\n",
    "        data['mean_embed']= data['tweet'].map(func)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_vectors(data, file_path, ending=None):\n",
    "    \n",
    "    # Get word vectors information\n",
    "    word_vectors = data[['label', 'mean_embed']].reset_index()\n",
    "    dim = len(word_vectors.loc[0, 'mean_embed'])\n",
    "    \n",
    "    # Name file to format : path/word_vectors_xxnsamplesxx_xxembeddimxx_ending_npy\n",
    "    name = file_path + 'word_vectors_' + str(data.shape[0]) + '_' + str(dim)\n",
    "    # Add ending if requested\n",
    "    if ending is not None :\n",
    "        name = name+'_'+str(ending)\n",
    "    \n",
    "    # save data\n",
    "    np.save(name, word_vectors)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(data, test_size=0.25) : \n",
    "\n",
    "    # Split into training and testing data\n",
    "    train, test = train_test_split(data, test_size=test_size)\n",
    "    print(f\"Local training set size : {train.shape[0]}.\")\n",
    "    print(f\"Local testing set size : {test.shape[0]}.\\n\")\n",
    "     \n",
    "    # Create features and labels datasets\n",
    "    xtrain, ytrain = np.array(train.mean_embed.tolist()), np.array(train.label.to_list())\n",
    "    xtest, ytest = np.array(test.mean_embed.tolist()), np.array(test.label.tolist())\n",
    "\n",
    "    print(f'Training sample shape: {xtrain.shape[1:]}')\n",
    "    print(f'Testing sample shape: {xtest.shape[1:]}')\n",
    "    \n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "def preprocess(X) :\n",
    "    x=X.copy()\n",
    "    \n",
    "    ## Standardize data\n",
    "    # If features are list of features of different sizes, standardization has to be made list-wise\n",
    "    if len(np.array(x).shape) == 1 : \n",
    "        x = np.array([ StandardScaler().fit_transform(a) for a in x ])\n",
    "    else :\n",
    "        x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    ## TODO Polynomial features and interactions\n",
    "    \n",
    "    ## other data preprocessing\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To format the testing data\n",
    "def extract_tweet(tweet):\n",
    "    return tweet.split(\",\", 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(data_path, test_file='test_data.txt') :\n",
    "    # Load the testing data\n",
    "    test = pd.read_fwf(data_path+test_file, sep=\"\\n\", header=None)\n",
    "    test = test.rename(columns={0:'tweet', 1:'na1', 2:'na2'})\n",
    "\n",
    "    # Reformating it for submission\n",
    "    test.index = test.index+1 # Format asked by AI Crowd\n",
    "    test = test['tweet'].map(extract_tweet).to_frame()\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating submission file\n",
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base parameters\n",
    "\n",
    "### Loading\n",
    "data_path = '../data/'\n",
    "pos_data = 'train_pos_full.txt'\n",
    "neg_data = 'train_neg_full.txt'\n",
    "\n",
    "precomputed_path = '../precomputed_data/'\n",
    "embeddings_file = 'embeddings_full_10epoch_100dim.npy'\n",
    "vocab_loc = '../data/vocab_cut.txt'\n",
    "\n",
    "embeddings_dim_info = 100\n",
    "data_size = 100_000\n",
    "\n",
    "word_vectors_files = ['word_vectors_100000_100_part1.npy',\n",
    "                     'word_vectors_100000_100_part2.npy']\n",
    "divided_word_vectors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded POS data, correctly interpreted 1-tweet-per-line fashion : True\n",
      "Loaded NEG data, correctly interpreted 1-tweet-per-line fashion : True\n",
      "Number of tweets : (POS) 1250000 (NEG) 1250000\n",
      "\n",
      "Loaded word embeddings in structure of type <class 'numpy.ndarray'>.\n",
      "Loaded word vocabulary in structure of type <class 'pandas.core.series.Series'>.\n",
      "\n",
      "Both the embeddings and the vocabulary are same length :  True\n",
      "Embeddings: (101298, 100), vocab: (101298,)\n",
      "NaN values were dropped in both tables: True\n",
      "Embeddings: (101296, 100), vocab: (101296,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "tweets = load_data(data_path, pos_data, neg_data)\n",
    "embeddings, vocab = load_embeddings_and_vocab(embeddings_file, precomputed_path, vocab_loc)\n",
    "\n",
    "# Resample data\n",
    "tweets_ = resample_data(tweets, data_size, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from divided dataset...\n",
      "Successfully loaded from file!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load word vectors\n",
    "tweets_ = load_word_vectors(\n",
    "    precomputed_path, \n",
    "    word_vector_files, \n",
    "    divided=divided_word_vectors)\n",
    "\n",
    "# Useless here (for demonstration purposes in case of computation and not loading)\n",
    "save_word_vectors(tweets_, precomputed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local training set size : 75000.\n",
      "Local testing set size : 25000.\n",
      "\n",
      "Training sample shape: (100,)\n",
      "Testing sample shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split data\n",
    "xtrain_, ytrain_, xtest_, ytest_ = split_dataset(tweets_)\n",
    "\n",
    "# Pre-process training set\n",
    "xtrain_ = preprocess(xtrain_)\n",
    "xtest_ = preprocess(xtest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions to estimate model efficiency\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for Linear Model is 0.03472532355713376.\n",
      "Wall time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Linear Regression\n",
    "name = 'Linear Model'\n",
    "\n",
    "linear_classifier = LinearRegression().fit(xtrain_, ytrain_)\n",
    "score = linear_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (linear_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for Logistic Regression is 0.57756.\n",
      "R2 score for Logistic Regression model using cross-validation is 0.57744.\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Logistic Regression\n",
    "name = 'Logistic Regression'\n",
    "\n",
    "logistic_classifier = LogisticRegression().fit(xtrain_, ytrain_)\n",
    "score = logistic_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (logistic_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} is {score}.\")\n",
    "\n",
    "# Logistic Regression using Crossvalidation\n",
    "name = 'Logistic Regression using cross-validation'\n",
    "\n",
    "logisticCV_classifier = LogisticRegressionCV().fit(xtrain_, ytrain_)\n",
    "score = logisticCV_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (logisticCV_classifier, score)\n",
    "\n",
    "print(f\"R2 score for Logistic Regression model using cross-validation is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos    0.56583   0.68774   0.62086     12573\n",
      "         neg    0.59601   0.46608   0.52310     12427\n",
      "\n",
      "    accuracy                        0.57756     25000\n",
      "   macro avg    0.58092   0.57691   0.57198     25000\n",
      "weighted avg    0.58083   0.57756   0.57226     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "predtest_ = logistic_classifier.predict(xtest_)\n",
    "\n",
    "report = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos    0.56570   0.68790   0.62085     12573\n",
      "         neg    0.59592   0.46568   0.52281     12427\n",
      "\n",
      "    accuracy                        0.57744     25000\n",
      "   macro avg    0.58081   0.57679   0.57183     25000\n",
      "weighted avg    0.58072   0.57744   0.57211     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "predtest_ = logisticCV_classifier.predict(xtest_)\n",
    "\n",
    "report = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for SVM classifier model is 0.57744.\n",
      "Wall time: 35.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Support Vector Machines\n",
    "name = 'SVM classifier'\n",
    "\n",
    "SVM_classifier = LinearSVC().fit(xtrain_, ytrain_)\n",
    "score = SVM_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (SVM_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} model is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos    0.56659   0.67979   0.61805     12573\n",
      "         neg    0.59395   0.47389   0.52717     12427\n",
      "\n",
      "    accuracy                        0.57744     25000\n",
      "   macro avg    0.58027   0.57684   0.57261     25000\n",
      "weighted avg    0.58019   0.57744   0.57287     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "predtest_ = SVM_classifier.predict(xtest_)\n",
    "\n",
    "report = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for Neural Network classifier is 0.60476.\n",
      "Wall time: 1min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### BASELINE : Neural Networks\n",
    "\n",
    "# Neural Network\n",
    "name = 'Neural Network'\n",
    "\n",
    "nn_classifier = MLPClassifier().fit(xtrain_,ytrain_)\n",
    "score = nn_classifier.score(xtest_,ytest_)\n",
    "\n",
    "classifiers[name] = (nn_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} classifier is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos    0.60043   0.64002   0.61960     12573\n",
      "         neg    0.60976   0.56908   0.58872     12427\n",
      "\n",
      "    accuracy                        0.60476     25000\n",
      "   macro avg    0.60510   0.60455   0.60416     25000\n",
      "weighted avg    0.60507   0.60476   0.60425     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "predtest_ = nn_classifier.predict(xtest_)\n",
    "\n",
    "metrics = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5, output_dict=True)\n",
    "report = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline results\n",
    "path = '../results/'\n",
    "name = path+'metrics_'+str(local_t_size)+'_'+str(embeddings_dim_info)+'_baseline'\n",
    "np.save(name, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom models\n",
    "Let us implement a custom model following the paper $\\text{Text Classification with Deep Neural Networks}$ by *Maaz Amajd et al.* from Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recompute word vectors datasets\n",
    "The convolutionnal Neural Network uses a matrix as input format. Here we use a matrix of words vectors representing the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load word vectors from file...\n",
      "Recomputing word vectors...\n",
      "Process is not run as admin. Cannot run parallelized setting, running as sequential...\n"
     ]
    }
   ],
   "source": [
    "tweets_cnn = get_word_vectors(tweets_, embedding_method='BOW', recompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>mean_embed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1193381</th>\n",
       "      <td>i cannot wait to party with &lt;user&gt; tonight</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.006390639092777149, -0.002517977760506535,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644929</th>\n",
       "      <td>haha she talking shit but don't know me . gosh...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.095365011208915, -0.002271404137185445, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578303</th>\n",
       "      <td>&lt;user&gt; excellent , as long as he keeps a bit o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.021176976452678558, 0.007691384674272959, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525309</th>\n",
       "      <td>&lt;user&gt; i'll be in mi on sunday ... i want to c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.021176976452678558, 0.007691384674272959, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34941</th>\n",
       "      <td>getting told you look like the person that dri...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.07722609727291953, -0.015988683796022186, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159713</th>\n",
       "      <td>&lt;user&gt; hey ! ! what time's your recital ? turn...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.021176976452678558, 0.007691384674272959, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249113</th>\n",
       "      <td>i miss being a kid . no one cared how you dres...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.006390639092777149, -0.002517977760506535,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913236</th>\n",
       "      <td>just cannot understand how to work tumblr prop...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.06250523629347589, -0.01246092651515487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830941</th>\n",
       "      <td>the trail of tears ( events that shaped americ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-0.002497237240290465, 0.0335195033127824, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34871</th>\n",
       "      <td>received an mms from maxis . ' my heart is loc...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.7619069241897088, 0.2361159544230768, -0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet label  \\\n",
       "index                                                              \n",
       "1193381         i cannot wait to party with <user> tonight     1   \n",
       "644929   haha she talking shit but don't know me . gosh...     1   \n",
       "578303   <user> excellent , as long as he keeps a bit o...     1   \n",
       "525309   <user> i'll be in mi on sunday ... i want to c...     1   \n",
       "34941    getting told you look like the person that dri...     1   \n",
       "...                                                    ...   ...   \n",
       "1159713  <user> hey ! ! what time's your recital ? turn...    -1   \n",
       "249113   i miss being a kid . no one cared how you dres...    -1   \n",
       "913236   just cannot understand how to work tumblr prop...    -1   \n",
       "830941   the trail of tears ( events that shaped americ...    -1   \n",
       "34871    received an mms from maxis . ' my heart is loc...    -1   \n",
       "\n",
       "                                                mean_embed  \n",
       "index                                                       \n",
       "1193381  [[0.006390639092777149, -0.002517977760506535,...  \n",
       "644929   [[0.095365011208915, -0.002271404137185445, 0....  \n",
       "578303   [[0.021176976452678558, 0.007691384674272959, ...  \n",
       "525309   [[0.021176976452678558, 0.007691384674272959, ...  \n",
       "34941    [[0.07722609727291953, -0.015988683796022186, ...  \n",
       "...                                                    ...  \n",
       "1159713  [[0.021176976452678558, 0.007691384674272959, ...  \n",
       "249113   [[0.006390639092777149, -0.002517977760506535,...  \n",
       "913236   [[0.06250523629347589, -0.01246092651515487, 0...  \n",
       "830941   [[-0.002497237240290465, 0.0335195033127824, -...  \n",
       "34871    [[0.7619069241897088, 0.2361159544230768, -0.2...  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_word_vectors(tweets_cnn, ending='cnn')\n",
    "tweets_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local training set size : (75000, 3).\n",
      "Local testing set size : (25000, 3).\n",
      "\n",
      "Training set first sample:\n",
      " [[0.03874826280722202, -0.0226154273381748, 0.06331501800493958, -0.05300686099727286, -0.02604800050298649, -0.06584109852584337, -0.008828779663752315, -0.10206945408357572, -0.00353245552938762, -0.029851338185586506, -0.0332773668685895, -0.0772952293011356, -0.028125405154649213, -0.07623803516338871, 0.02133121024922275, 0.02422468594373341, 0.01366152352805924, -0.02749200397469116, 0.09053584800823926, -0.023833276378709092, 0.004043477932097031, -0.05621087884414685, 0.0526542836005664, 0.045542285624793066, -0.033528027208590556, -0.017383583133306613, -0.03443335386155496, -0.10966482189803363, 0.04206032044073065, -0.07445478974047048, 0.007670329872138221, 0.0509657715676167, -0.014440089971403428, 0.11515830005637248, 0.10204113604695303, -0.04486226197048761, 0.0791685780560366, 0.025574548862244122, -0.045750544964075976, 0.0499636137507949, 0.03762462946514737, 0.043730395505155895, 0.04042619453658016, 0.017827619406573566, 0.19176841335987221, -0.10167572891057736, 0.03661949815765191, -0.010080986086904997, -0.00043321925557569764, 0.023427650270941316, 0.07844131990371339, -0.07034712299576808, 0.03782711965822852, -0.09519614905896609, 0.009965385624857272, -0.03385306864796681, 0.028001230037181447, -0.05716964830084966, 0.10912491784440964, -0.044073634639243194, 0.009529818356313027, -0.01743463866427918, 0.04312266979171952, -0.005398493699931578, -0.07738709849223772, -0.06390053399426142, -0.018053386382628368, -0.02843255580277417, 0.011263346488290516, 0.011847996506057301, -0.11803265665910766, -0.08123417958207392, 0.013421684691729382, -0.13854675456210064, 0.036650607234384375, 0.017695919438092268, -0.06646973924601096, 0.053211119784353375, 0.013474056780384377, -0.04126979786365878, -0.00959953855138115, -0.1029459182552377, 0.014743141533963751, -0.020117150336572957, -0.010700014460789593, -0.07595102547034846, -0.0207070806318504, 0.022023765724119963, -0.017998417378203912, -0.011970413917000756, -0.09077017031471867, -0.08566726060697527, 0.14645275147005818, -0.14223984250592392, 0.05566548757570775, -0.027030644171744538, 0.06319442594210875, -0.06277219930595619, 0.020477970696876277, -0.027097211318444503], [0.05820608759052884, 0.0022495328327372787, 0.05592900231709098, -0.07272714351321583, -0.011139520771943238, -0.07197715599126948, 0.01824578946921668, -0.14154678959127096, -0.00961572511579202, -0.04654448666806063, -0.044890793351426435, -0.09964836474909314, -0.0214941979998027, -0.07894129783545852, 0.02164564763259233, 0.04289817113236922, 0.00631849209159897, -0.017618329146167976, 0.13358727311270682, -0.03825997471605937, 0.012023670981589319, -0.07877692346385555, 0.07251325417322471, 0.060973751361367026, -0.07236442834864838, -0.02302867804603923, -0.07603134167986773, -0.15629166587235424, 0.06970779318031717, -0.059988513430677355, -0.013560615080073285, 0.06370665105287864, 0.023091199483345634, 0.18497032655685888, 0.15244875328284055, -0.09027318972360529, 0.10245313005966954, 0.0535873181125109, -0.09659701096046538, 0.10783268541216598, 0.038790248908239254, 0.039744460666295015, 0.08900346384853654, 0.04249945595610479, 0.2794367731259615, -0.1710077670830055, 0.02724512724517111, -0.016658664272653938, 0.0011378770765326012, 0.04699576477543221, 0.13509798224906747, -0.06851684314536104, 0.03774614163037288, -0.1607475406541802, 0.009663668655153492, -0.061538855465913415, 0.03976635800594569, -0.1015704546278467, 0.1428398025490679, -0.07686275450925178, 0.020259780522727196, -0.04921360649107523, 0.04197825208437956, -0.01523737128419591, -0.07852210805132083, -0.07251412382274655, -0.04772270700061804, 0.004564681077092937, -0.0015970195958259846, 0.03261244235519234, -0.16772304044204112, -0.15134153065255146, 0.029968944332917507, -0.21935761920010377, 0.08336773641575977, 0.03239715524241938, -0.056717222463693574, 0.059080234635765706, 0.01584215095941949, -0.0844248378849269, -0.0028686903138194827, -0.12412097537784451, -0.022350418354654566, -0.02485676976163906, -0.0029115821461034685, -0.12095931811397967, -0.027121567560003543, 0.06530838983210088, -0.03349178702642274, -0.006951111641130268, -0.15598208557837057, -0.11790873983491176, 0.2195237103862889, -0.20751254986142428, 0.049924984732186366, -0.06340375744226763, 0.07229356218703047, -0.11771285891955358, 0.01805332206872844, -0.054492086282707144], [-0.19216937896752379, -0.18459381231553879, 0.15021902608889381, -0.15036323717040487, -0.23473951771613363, -0.15019933576268427, -0.027707246703501852, -0.35850633916351066, -0.043643958519660596, -0.08987065385048998, 0.08168295882905287, -0.1109036429055195, 0.2377179688944711, -0.06899924889212158, 0.22946386297252824, -0.15740330925828952, 0.16133105521039842, -0.0899297437185177, 0.08397439551877403, 0.03849267075430831, 0.006682631054480497, -0.2244923596571173, -0.04350558044305328, 0.034697818881145356, -0.20160148838473133, -0.13141442522649976, -0.2731807824675881, -0.16727427471231257, 0.1689869989518395, -0.13019254100150024, 0.23027326470788537, 0.23973484813077459, -0.13163571385401163, 0.7331070019572344, 0.25449493737720563, -0.06276526187685744, 0.09246462396615954, 0.06448362768893984, -0.3456522703605619, 0.09371686998972005, -0.13202724434786234, 0.027481924320620386, 0.030483154043455658, 0.1057387954362006, 0.6051127211843214, -0.39886431568519004, -0.1547312815814569, -0.07596103828012493, 0.21203332627721272, 0.02835721157776318, 0.1826751960966946, 0.12952593624328662, 0.23335543728848177, -0.2982319420557599, 0.1310649265573444, -0.17729141195799886, 0.10796266375664929, -0.2357612775405139, -0.07813163791339375, -0.05846707707131342, -0.03242320266628979, -0.32843927854938504, 0.2924589003074826, -0.14911148707414143, -0.11649231923600002, 0.08840137981938635, -0.20292660532102605, 0.0005167189763296708, 0.08681768349547866, 0.14405718443954774, -0.46087998191549884, -0.35055590058146535, -0.179967083290669, -0.3652962362823989, 0.10904926360091603, 0.06775759848869775, -0.2134168531414477, 0.057733452436219454, -0.0677871042262281, -0.1262399758761953, -0.23209938045935735, -0.2911171915012088, 0.12765461575578588, 0.09501104239291536, -0.13984976014494635, -0.19174593746116045, 0.1316784475458429, -0.1992135178569957, -0.1467788498688385, -0.10377572468814714, -0.1966713272331518, -0.11179973541521511, 0.056226028731285675, -0.3159408473868224, -0.01937181933698707, -0.24896044875699636, 0.011853059469522947, -0.16382287260879178, 0.053336293192484924, -0.10739986070856991], [0.0027545743641906144, 0.0052100503881193794, 0.0026322899284604057, 0.0035652504056979863, 0.060147497836464066, -0.015125914185375865, 0.0060059212489176495, -0.009220229770744649, 0.0039360493991093615, -0.0016001279475589352, -0.006781617371801093, 0.04832437587494029, 7.880960000203885e-05, -0.003243144469023905, 0.017678517956893697, 0.002484997176322467, 0.01661562625545125, -0.006532931797656542, 0.00455803772731901, 0.03830456391135628, 0.007634838950340619, 0.003577651815269952, -0.008058567167417097, -0.020774347087562612, -0.013242112282120673, -0.030354361250925967, -0.04247067447842525, -0.015296551174782905, 0.04163414468959251, -0.010455948217622671, 0.012134500353266587, -0.006352530178640601, -0.04341526316912584, 0.02400173612692962, 0.0205661973367328, 0.025076495497212473, -0.030179661238226557, 0.004658497762236279, -0.027115285206692194, 0.02328340782122058, 0.014069077170833784, -0.021187441359492198, -0.0037566804804181012, -0.0032951404148482345, 0.033298770177679224, 0.0210194676655666, -0.0007275164355316272, 0.003916440792351968, 0.0014695411763435322, -0.019807136891309713, -0.010160279090746044, 0.0003567272797723158, 0.06185841820193822, 0.016043446489968475, 0.042723530825558534, -0.0009514098177313903, 0.02369010952896426, -0.013249428407011016, 0.015366909766182138, -0.01510852795017399, 0.006275362371251281, 0.039008180820299665, -0.0275175425505643, 0.01229724462332223, -0.026962389267507037, 0.026144073249363915, -0.022907872346236122, -0.01592486846567347, 0.01167292965700065, 0.02049593387189821, 0.006823601103504785, -0.022794209911539846, 0.007079818480414288, 0.0022822206138110304, 0.0004197265363002411, 0.005605468383396001, -0.026675561984632837, 0.03390913322950142, -0.004672655134847359, -0.0179084458304655, -0.032255748622730965, 0.012086025813572158, -0.008232146307451392, -0.026699741077308367, 0.0004902798547477128, 0.003955720720034941, 0.022426956063241132, 0.0007790969622678728, 0.0031504755167318507, 0.0007554888404524806, -0.009073752284675229, -0.03061165418539486, -0.0013534472449101991, -0.02075989026297182, 0.01765257319583058, 0.042277956247533145, 0.0043655003936694465, 0.02477620639528626, -0.03013439267912033, 0.009534976941133233], [0.07515433184050824, -0.02924703695566412, 0.13354649385819214, -0.14519354490168107, -0.08009635938363278, -0.175190224796296, -0.0013535406959149336, -0.25458765032929503, -0.021503923901046326, -0.08179496543800516, -0.0816332301169702, -0.20164997451911118, -0.04855552278859872, -0.1771123293652343, 0.04008214009888767, 0.09909152481963737, 0.015813866818656687, -0.037713993300144216, 0.2497454712985588, -0.08415147516594096, 0.028195932501263912, -0.15919535656573855, 0.10738059167192957, 0.13542311793280243, -0.0742185998528798, -0.0348175386384035, -0.12163187475109939, -0.2688108834883237, 0.11131688146361553, -0.13447733565274403, -0.0028020129631008686, 0.11564654403515874, -0.01018498982212832, 0.36129406033750766, 0.2908383839384728, -0.1748474518813453, 0.200134008539612, 0.09179789217089111, -0.18522673005412937, 0.1572973652212067, 0.0778416213116179, 0.11738599934888744, 0.12704615286575802, 0.0525361968715133, 0.5286329838265903, -0.30238931546577225, 0.067259499541434, -0.042619284159097055, 0.011438450864488876, 0.08131161907671326, 0.25075061364392043, -0.10662083730623212, 0.06763996436603019, -0.2880780197875056, 0.04881645594082596, -0.09230435630792991, 0.03624801866647685, -0.13382957666942083, 0.24255346608589473, -0.10142349576097076, 0.05482952664679435, -0.06548941602467875, 0.12707105646037337, -0.02487226633512448, -0.18782692064410547, -0.12138910143106774, -0.09550845288575299, -0.012798845723220818, -0.03902109882307062, 0.08131361057783822, -0.285603593858939, -0.2174291645511087, 0.03507193688086718, -0.371069710897142, 0.16291978386682063, 0.07318975501195732, -0.1344472888410358, 0.11809333736038383, 0.0518470698114975, -0.1518363025414729, 0.027521430314695523, -0.250782350484579, -0.022131931967478643, -0.025881474057089765, -0.025679276761260014, -0.21816145783110966, -0.03856456976204739, 0.06649501983576563, -0.030052430661722935, -0.007960688187052412, -0.2337733342820218, -0.24231773790923417, 0.4194212128172572, -0.34608104699595627, 0.09742841909650903, -0.0616134979688927, 0.12840184780056604, -0.21306758179902024, 0.03109192999913905, -0.09863127447132354], [0.0691572792807175, -0.01312411951679047, 0.13980825118309378, -0.11186770811240329, -0.0605956913197638, -0.15540948632245896, -0.013862032113596203, -0.2478336241363276, -0.043760348431526874, -0.08713180673177463, -0.06114522664096007, -0.150113768597251, -0.05821713389652822, -0.14533117930980619, 0.06117725060671246, 0.11494250914963286, 0.003760783196208864, -0.04960770516027261, 0.20222142837538887, -0.07382959857669802, 0.026169674913899054, -0.15312512751808344, 0.12520470433204997, 0.115476272125863, -0.07251603233816889, -0.024307016358051648, -0.09179296418247435, -0.22022397444844227, 0.05885584331124946, -0.11534945970089835, -0.019161038983485524, 0.09255171514670799, -0.003690490823252037, 0.3104021193291385, 0.27053021054972565, -0.14066657255936638, 0.20768370585870136, 0.07117919860377206, -0.184577343629972, 0.16707879438809306, 0.03643113142088478, 0.12826695732220667, 0.09587743479374763, 0.02914636551400659, 0.49202219249715806, -0.28016887974956, 0.05848876565649934, -0.049291906919303366, 0.0026641416209873684, 0.08107055388950164, 0.2544917551355878, -0.09684555617467717, 0.08770072096153733, -0.2382169933165747, 0.02088551504516912, -0.08189262063872174, 0.03792250659856985, -0.15426787296208624, 0.27525439518543227, -0.1192251657072366, 0.07193045742793748, -0.06091230840302329, 0.0878200537203543, -0.01723978085377881, -0.1600115864309396, -0.11317436958745633, -0.09570478656532823, 0.02527229384311331, -0.018370297212873813, 0.042124080715243706, -0.260674935331846, -0.20905992571591325, 0.04529761858962358, -0.31725647973434157, 0.13488382686439998, 0.02552431950972449, -0.12279547082774225, 0.10702094439195677, 0.04152306903989739, -0.14189104301216277, 0.003925914256898108, -0.20864136149742857, -0.00784566674122313, -0.023692047317318582, 0.026020286228551126, -0.17395982180103295, -0.0029453170623644992, 0.07479381183741815, -0.03148392040474831, 0.014238975525189967, -0.23291790872667245, -0.18291581118475303, 0.34496081310295146, -0.3052190985778598, 0.08856010385119713, -0.043278552651283886, 0.11976446240003111, -0.19594650182122955, 0.03855609978056234, -0.049601271487643414], [0.1020566985999923, -0.006643052749302242, 0.12886555371782257, -0.1246477806248689, -0.08233098386589374, -0.17501299293283237, -0.0102689764133075, -0.2983172579780782, -0.02040236981203195, -0.08982749399580733, -0.09678860723766046, -0.2312725626495482, -0.10214588038933738, -0.17564662141382364, 0.08207473773394003, 0.07415257626038618, -0.00027792980141612953, -0.028892026414784975, 0.280921764925327, -0.0830237192242573, 0.01816744873539309, -0.1801963450421498, 0.14990500267771367, 0.13318930931310763, -0.10551009519616805, -0.04538893412310743, -0.09833183309807543, -0.28305074351332216, 0.1363066461755239, -0.1206897152398501, 0.026856164361743, 0.10149807861480732, -0.02306120039145482, 0.38016586407690006, 0.31703165253764454, -0.18568296433056866, 0.20789585948441794, 0.10777644691374506, -0.20031586944073929, 0.16424325803405876, 0.08344476846832989, 0.139804781882901, 0.09427555611452043, 0.05736064277165888, 0.5246164526851739, -0.3258671214555689, 0.05666357859194896, -0.07984526610210543, -0.009082962230222088, 0.10619611050692114, 0.2784471628415889, -0.09313977789648975, 0.12551120106556787, -0.30541449894822775, 0.0024132499728285774, -0.13556530307209308, 0.06149030867239727, -0.1605451106304157, 0.2768531569390484, -0.12833808043044054, 0.030869350922570878, -0.09498712937267563, 0.1583352389603186, -0.027987332954837973, -0.19493236405030911, -0.15672411997993793, -0.09113128795284348, 0.024472224982630464, -0.0488950308494302, 0.07945816120951997, -0.30469960844858957, -0.2283589390724489, 0.07258572975303082, -0.4193445504007812, 0.12150360789712006, 0.022448168786341514, -0.16288583223675604, 0.10436540471117627, 0.039268019643212033, -0.1705067832997982, 0.00295945419735046, -0.2951451460136482, -0.037581438975144967, -0.07220620133890092, -0.024012468459885427, -0.19680691657724556, -0.032120657156750936, 0.07492804412642952, -0.040204112020949964, 0.006368280603579507, -0.2584999722542277, -0.22424952562065031, 0.4374617426258517, -0.34228788590706855, 0.12117074749416404, -0.06346676463311088, 0.1617071740760636, -0.24609492818961332, 0.03661920209773757, -0.08697149591375485], [0.07436374983839178, -0.01905474268548291, 0.1287295608061761, -0.14517568960892055, -0.0609641796289088, -0.17411998812701157, 0.0014308455485270064, -0.2881914212498898, -0.029564060797347674, -0.08379444234507913, -0.07106732643351818, -0.16474914853344932, -0.06553352647473623, -0.1666260232867646, 0.036154636437664346, 0.08088942206128813, -0.009590457403524884, -0.034645466690874184, 0.24498421911471224, -0.09301173907363477, 0.04294546239781629, -0.16597473531133, 0.12110750424535538, 0.13720711313911405, -0.09820699403133422, -0.053886598975223854, -0.12305071154521291, -0.23983011910244492, 0.09909844676077793, -0.1605146398019212, 0.015480907964199946, 0.11729972881714394, -0.00959894930574319, 0.3486474211183854, 0.2742059304922802, -0.14782675181477611, 0.18431264662756586, 0.08363335399260709, -0.19129697991644148, 0.17762229662428622, 0.05257116680618082, 0.13453128571412115, 0.12142972781738523, 0.0408730027895796, 0.5038265154284864, -0.31160179693816914, 0.08935470786659128, -0.0712815613729251, 0.020834645235876047, 0.09563117745438847, 0.241501641832257, -0.11164365420221284, 0.08170126556102061, -0.2635914851531453, 0.061768830786005, -0.11862241876001176, 0.06296072956630143, -0.17200627106799798, 0.2641718111096616, -0.0988902282708519, 0.06061102717482227, -0.06695321700275939, 0.12161387437013318, -0.02172315300523183, -0.16751303986698662, -0.13487530470254913, -0.10032381808451857, 0.027059841502697836, -0.042071654497703566, 0.06606971578247917, -0.2937111308587792, -0.2253918074017661, 0.054335299944311376, -0.3448204367964576, 0.16012794006978656, 0.04590275850111483, -0.12903324112757844, 0.1265949675057719, 0.030850377909221995, -0.1428161738508605, 0.029228284310985904, -0.23916210699217227, -0.03513267722252979, -0.0337868208701595, -0.026980248887626273, -0.16348631145973197, -0.01975403989205712, 0.06538101386754905, -0.04070246200596251, -0.0013059177219584277, -0.25492325012054046, -0.2170908219936803, 0.4079396911159087, -0.33552267466404395, 0.10776384913680849, -0.06631093098517676, 0.13572636138830188, -0.21056665536035094, 0.02538433696240985, -0.07072058728245628], [0.07334670937383486, -0.009654711612605392, 0.12421537339038405, -0.11491016261683643, -0.06466623430213514, -0.1672374176929014, -0.03175249581954941, -0.2435428303715889, -0.006852770194524391, -0.11247588582860132, -0.08138399674390809, -0.20880150618744756, -0.08716029552615691, -0.20229922681829285, 0.07407229673592022, 0.1240687284862981, -0.012842009952032206, -0.069309308997094, 0.26274962575945376, -0.09756491376361412, 0.018213114607896663, -0.1831358733773425, 0.155138859132073, 0.14837049866190746, -0.09282620104717827, -0.09182426259114818, -0.08191174537009167, -0.2506024882019347, 0.13163193079739036, -0.13782337623537355, 0.000230741298244809, 0.08269668944388867, -0.033940187554852, 0.3644445771788002, 0.3180484281408125, -0.13071536905215075, 0.21346458504291704, 0.08004071912297321, -0.21972660849204456, 0.14474242853277883, 0.109551243674322, 0.11138756720322274, 0.11622285356049354, 0.05882104342504072, 0.5040295630092743, -0.3208957546580599, 0.0815043940801417, -0.05898679559412221, 0.008307753084080062, 0.11422261805303853, 0.26745441498209094, -0.11828774035711155, 0.09968951902465976, -0.26680658766597504, 0.03825594951741425, -0.14281862119843805, 0.03816729666964381, -0.16637698840052778, 0.268468860282916, -0.1451512235856671, 0.08788846851662617, -0.03173599167456553, 0.1579638762901177, 0.006730172130426026, -0.18658423943727212, -0.14079508069032678, -0.08479283222415598, 0.008064810001472578, -0.06809956145804914, 0.07724409000676258, -0.31077479857969703, -0.2462232497840166, 0.016934751275728528, -0.34814264465284867, 0.1330808458259868, 0.06536028278495862, -0.14784481904892458, 0.08085523843014383, 0.032310920661306954, -0.1802617472559888, 0.050449261295665404, -0.24262446149460995, -0.05814119758075909, -0.04238113675035681, -0.003343272050174639, -0.1594878059233816, -0.025659403985406035, 0.08901667432781366, -0.0414383971399819, 0.007442077684993643, -0.2663067945467369, -0.22905691985610876, 0.4421534056631639, -0.3113627727494416, 0.12821752115372242, -0.0569278811083221, 0.14054699498613005, -0.18990956081864382, 0.025654375024349942, -0.06062893272119833]]\n"
     ]
    }
   ],
   "source": [
    "xtrain_cnn, ytrain_cnn, xtest_cnn, ytest_cnn = prepare_data(tweets_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "# Define base parameters for the CNN\n",
    "\n",
    "batch_size = 64 ## TODO tune batch size (depends on mem)\n",
    "epochs = 20\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_, ytrain_ = np.array(xtrain_), np.array(ytrain_)\n",
    "xtest_, ytest_ = np.array(xtest_), np.array(ytest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_17 (Conv1D)           (None, 75000, 32)         6432      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 75000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 37500, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 37500, 64)         4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 37500, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 18750, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 18750, 128)        16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 18750, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 9375, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1200000)           0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               153600128 \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 153,627,490\n",
      "Trainable params: 153,627,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the layers of the CNN\n",
    "cnn_classifier = Sequential()\n",
    "cnn_classifier.add(Conv1D(32, kernel_size=2,activation='relu',input_shape=xtrain_.shape[] ,padding='same'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))\n",
    "cnn_classifier.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "cnn_classifier.add(Conv1D(64, 2, activation='relu',padding='same'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))\n",
    "cnn_classifier.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "cnn_classifier.add(Conv1D(128, kernel_size=2, activation='relu',padding='same'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))                  \n",
    "cnn_classifier.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "cnn_classifier.add(Flatten())\n",
    "cnn_classifier.add(Dense(128, activation='linear'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))                  \n",
    "cnn_classifier.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "cnn_classifier.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "cnn_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 100]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 100]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "cnn_history = cnn_classifier.fit(xtrain_, ytrain_, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(xtest_, ytest_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CNN results\n",
    "path = '../results/'\n",
    "name = path+'metrics_'+str(local_t_size)+'_'+str(embeddings_dim_info)+'_CNN'\n",
    "np.save(name, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "This section is dedicated to using the previous classifiers to predict the labels of the provided testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>mean_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1193381</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.032002123948292126, -0.014642657203437489, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>644929</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.10238272539678156, -0.02736877932503185, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578303</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.032941437860502285, 0.0014634508830161917, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>525309</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04879550502681794, 0.017707842326910788, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34941</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1420258276670681, -0.03843481256309387, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1159713</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.15079636217861106, 0.005626630631020118, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>249113</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.05451928253254213, -0.052474023026710545, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>913236</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.09112505455684311, 0.040824608634503316, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>830941</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.12102069953544096, -0.047875157832210066, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>34871</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.12092917762249158, -0.06853298903401578, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index label                                         mean_embed\n",
       "0      1193381     1  [0.032002123948292126, -0.014642657203437489, ...\n",
       "1       644929     1  [0.10238272539678156, -0.02736877932503185, 0....\n",
       "2       578303     1  [0.032941437860502285, 0.0014634508830161917, ...\n",
       "3       525309     1  [0.04879550502681794, 0.017707842326910788, 0....\n",
       "4        34941     1  [0.1420258276670681, -0.03843481256309387, 0.0...\n",
       "...        ...   ...                                                ...\n",
       "99995  1159713    -1  [0.15079636217861106, 0.005626630631020118, 0....\n",
       "99996   249113    -1  [0.05451928253254213, -0.052474023026710545, 0...\n",
       "99997   913236    -1  [0.09112505455684311, 0.040824608634503316, 0....\n",
       "99998   830941    -1  [0.12102069953544096, -0.047875157832210066, 0...\n",
       "99999    34871    -1  [0.12092917762249158, -0.06853298903401578, 0....\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is not run as admin. Cannot run parallelized setting, running as sequential...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-415d7496b1fe>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if len(np.array(x).shape) == 1 :\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=-0.009954323422670053.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2c90f068df67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_word_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_embed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Neural Network'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-415d7496b1fe>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# If features are list of features of different sizes, standardization has to be made list-wise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-415d7496b1fe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# If features are list of features of different sizes, standardization has to be made list-wise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[0mTransformer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \"\"\"\n\u001b[1;32m--> 696\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m    697\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                                 force_all_finite='allow-nan')\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;31m# If input is scalar raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    613\u001b[0m                     \u001b[1;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=-0.009954323422670053.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "test = get_test_data('../data/')\n",
    "test = compute_word_vectors(test, embeddings, 100, vocab)\n",
    "xtest = preprocess(test.mean_embed.tolist())\n",
    "\n",
    "model = classifiers['Neural Network'][0]\n",
    "predictions = model.predict(xtest)\n",
    "create_csv_submission(test.index, predictions, '../submission/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models computed so far are the following.\n",
      " \n",
      "Classifier                                         | R2 Score            \n",
      "-----------------------------------------------------------------\n",
      "Linear Model                                       | 0.0528607913\n",
      "Logistic Regression                                | 0.5927200000\n",
      "Logistic Regression using cross-validation         | 0.5928000000\n",
      "SVM classifier                                     | 0.5940400000\n",
      "Neural Network                                     | 0.6162800000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recalling classifiers \n",
    "# stored in format : 'classifier name'=(classifier, R2 score) \n",
    "\n",
    "print(f\"Models computed so far are the following.\\n \")\n",
    "print(f\"{'Classifier':50s} | {'R2 Score':20s}\")\n",
    "print(f\"-----------------------------------------------------------------\")\n",
    "for k,v in classifiers.items() :\n",
    "    print(f\"{k:50s} | {v[1]:10.10f}\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Here we detail previously computed results from `results/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Dimensions evolution\n",
    "Below we detail how different metrics perform when the dimension of the word embeddings computed by GloVe algorithm changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 dimensions\n",
      "\n",
      "Accuracy : 0.585\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.574874 |   0.671121 |   0.619280\n",
      "neg        |   0.599399 |   0.497868 |   0.543936\n",
      "\n",
      "50 dimensions\n",
      "\n",
      "Accuracy : 0.5896\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.575301 |   0.671701 |   0.619775\n",
      "neg        |   0.609460 |   0.508167 |   0.554223\n",
      "\n",
      "100 dimensions\n",
      "\n",
      "Accuracy : 0.60904\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.587911 |   0.726698 |   0.649979\n",
      "neg        |   0.643141 |   0.491608 |   0.557257\n",
      "\n",
      "250 dimensions\n",
      "\n",
      "Accuracy : 0.61628\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.634955 |   0.542312 |   0.584988\n",
      "neg        |   0.602425 |   0.689859 |   0.643184\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "f1s = []\n",
    "\n",
    "\n",
    "for d in [20,50,100,250] :\n",
    "    print(f'\\n{d} dimensions\\n')\n",
    "    name = path+'metrics_'+str(local_t_size)+'_'+str(d)+'_baseline.npy'\n",
    "    r = np.load(name, allow_pickle=True)\n",
    "    r = r.item()\n",
    "    print(f\"Accuracy : {r['accuracy']}\")\n",
    "    print(f\"{'Label':10s} | {'precision':10s} | {'recall':10s} | {'f1-score':10s}\")\n",
    "    print(f\"{'pos':10s} | {r['pos']['precision']:10f} | {r['pos']['recall']:10f} | {r['pos']['f1-score']:10f}\")    \n",
    "    print(f\"{'neg':10s} | {r['neg']['precision']:10f} | {r['neg']['recall']:10f} | {r['neg']['f1-score']:10f}\")  \n",
    "    accs.append(r['accuracy'])\n",
    "    precs.append(r['pos']['precision'])\n",
    "    recs.append(r['pos']['recall'])\n",
    "    f1s.append(r['pos']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [20,50,100,250]\n",
    "plt.plot(x, accs)\n",
    "plt.plot(x, precs)\n",
    "plt.plot(x, recs)\n",
    "plt.plot(x, f1s)\n",
    "plt.legend(['accuracy','precision','recall','f1-score'])\n",
    "plt.xticks(ticks=np.arange(0,260,20))\n",
    "plt.vlines(190,ymin=0.53, ymax=0.725, color='black')\n",
    "name = '../plots/'+'metrics_'+str(local_t_size)+'_dims_baseline.png'\n",
    "plt.savefig(name, dpi=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the optimal number of dimensions seems to lie around **190** as the parameters are at their common highest at this point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
