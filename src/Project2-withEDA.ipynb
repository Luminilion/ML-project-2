{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base parameters\n",
    "\n",
    "### Loading\n",
    "# Data \n",
    "data_path = '../data/'\n",
    "\n",
    "\n",
    "embeddings_file = 'embeddings_full_10epoch_100dim.npy'\n",
    "embeddings_files = ['embeddings_full_10epoch_250dim_part1.npy', \n",
    "                    'embeddings_full_10epoch_250dim_part2.npy', \n",
    "                    'embeddings_full_10epoch_250dim_part3.npy']\n",
    "divided_embeddings = False\n",
    "embeddings_dim_info = 100\n",
    "vocab_file = 'vocab_cut.txt'\n",
    "\n",
    "# Training\n",
    "local_t_size = 100_000\n",
    "precomputed_file_location = '../precomputed_data/'\n",
    "word_vectors_files = ['word_vectors_100000_100_part1.npy',\n",
    "                     'word_vectors_100000_100_part2.npy']\n",
    "divided_word_vectors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading\n",
    "\n",
    "In this section we load the data for :\n",
    "* positive tweets, label= `:)` ($1$ for classification) \n",
    "* negative tweets, label= `:(` ($-1$ for classification)\n",
    "\n",
    "Full data is used below (1'250'000 tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, pos_file, neg_file, size=1_250_000): \n",
    "\n",
    "    # positive\n",
    "    pos = pd.read_table(data_path+pos_file, sep='.\\n', names=['tweet'], engine='python')\n",
    "    pos['label']=1\n",
    "    print(f\"Loaded POS data, correctly interpreted 1-tweet-per-line fashion : {pos.shape[0]==size}\")\n",
    "\n",
    "    # negative\n",
    "    neg = pd.read_table(data_path+neg_file, sep='.\\n', names=['tweet'], engine='python')\n",
    "    neg['label']=-1\n",
    "    print(f\"Loaded NEG data, correctly interpreted 1-tweet-per-line fashion : {neg.shape[0]==size}\")\n",
    "\n",
    "    # Data sizes\n",
    "    print(f\"Number of tweets : (POS) {pos.shape[0]} (NEG) {neg.shape[0]}\\n\")\n",
    "\n",
    "    # Merge datasets to get a complete training set\n",
    "    tweets = pos.append(neg)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded POS data, correctly interpreted 1-tweet-per-line fashion : True\n",
      "Loaded NEG data, correctly interpreted 1-tweet-per-line fashion : True\n",
      "Number of tweets : (POS) 1250000 (NEG) 1250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "tweets = load_data(data_path, 'train_pos_full.txt', 'train_neg_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove_helper import concatenate\n",
    "\n",
    "def load_embeddings_and_vocab(embeddings_loc, embeddings_path, vocab_loc, divided=False) :\n",
    "    \n",
    "    ## Load word embeddings and vocabulary to compute word vectors of tweets -----------------------------------\n",
    "    \n",
    "    # Load word embeddings\n",
    "    embeddings=None\n",
    "    if divided : \n",
    "        embeddings = concatenate(embeddings_files, embeddings_path)\n",
    "    else :\n",
    "        embeddings = np.load(embeddings_path+embeddings_loc)\n",
    "    print(f'Loaded word embeddings in structure of type {type(embeddings)}.')\n",
    "\n",
    "    # Loading vocab\n",
    "    words = pd.read_table(vocab_loc, sep='.\\n', names=['word'], engine='python', squeeze=True, na_values=np.nan)\n",
    "    print(f'Loaded word vocabulary in structure of type {type(words)}.')\n",
    "\n",
    "    # Check that the vocabulary encompasses all embedded words\n",
    "    print(f'\\nBoth the embeddings and the vocabulary are same length :  {len(embeddings)==words.shape[0]}')\n",
    "    print(f\"Embeddings: {embeddings.shape}, vocab: {words.shape}\")\n",
    "\n",
    "    ## Clean the data --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Drop NaN values in vocab\n",
    "    nas = words.isna()\n",
    "    words.dropna(inplace=True)\n",
    "    # Drop NaN words in embeddings\n",
    "    embeddings = np.delete(embeddings, nas[nas].index.values, axis=0)\n",
    "    \n",
    "    print(f'NaN values were dropped in both tables: {len(embeddings)==words.shape[0]}')\n",
    "    print(f\"Embeddings: {embeddings.shape}, vocab: {words.shape}\")\n",
    "\n",
    "    ## Process data ---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Index by words for faster index-for-word search\n",
    "    words = pd.DataFrame(data=words.index, index=words.values)\n",
    "    embeddings = pd.DataFrame(embeddings, index=words.index)\n",
    "    \n",
    "    return embeddings, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word embeddings in structure of type <class 'numpy.ndarray'>.\n",
      "Loaded word vocabulary in structure of type <class 'pandas.core.series.Series'>.\n",
      "\n",
      "Both the embeddings and the vocabulary are same length :  True\n",
      "Embeddings: (101298, 100), vocab: (101298,)\n",
      "NaN values were dropped in both tables: True\n",
      "Embeddings: (101296, 100), vocab: (101296,)\n"
     ]
    }
   ],
   "source": [
    "embeddings, vocab = load_embeddings_and_vocab(embeddings_file, precomputed_file_location, '../data/vocab_cut.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt;</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "<user>  0\n",
       "!       1\n",
       "i       2\n",
       "the     3\n",
       ".       4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory Data Analysis\n",
    "In this part we analyse our data in order to optimize its information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning tags \n",
    "Here we explore the non-spoken tags present in the tweets and determine if they are relevant for our sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|KEY           |POS   |NEG   |\n",
      "|<blink>       |     0|     1|\n",
      "|<summary>     |     1|     0|\n",
      "|<ages>        |     0|     1|\n",
      "|<emotional>   |     0|     1|\n",
      "|<here>        |     1|     0|\n",
      "|</script>     |     0|     4|\n",
      "|<syrian>      |     0|     6|\n",
      "|<thx>         |     1|     0|\n",
      "|<joke>        |     1|     0|\n",
      "|<user>        |1027205|578390|\n",
      "|<mikel>       |     1|     0|\n",
      "|<em>          |     0|     2|\n",
      "|<blushing>    |     1|     0|\n",
      "|<mournfully>  |     0|     1|\n",
      "|<please>      |     0|     1|\n",
      "|</i>          |     0|    11|\n",
      "|<trans>       |     0|     3|\n",
      "|<content>     |     0|     1|\n",
      "|<cutestuff>   |     2|     0|\n",
      "|<hot>         |     1|     0|\n",
      "|<screams>     |     0|     1|\n",
      "|<hahahahhahaha>|     0|     1|\n",
      "|</del>        |     0|     1|\n",
      "|<sciencestuff>|     2|     0|\n",
      "|<sarah>       |     1|     0|\n",
      "|<space>       |     3|     0|\n",
      "|<dynamic>     |     1|     0|\n",
      "|<weirdarms>   |     1|     0|\n",
      "|<impressive>  |     1|     0|\n",
      "|</popcorn>    |     1|     0|\n",
      "|</moan>       |     0|     1|\n",
      "|<naive>       |     0|     1|\n",
      "|<c>           |     1|     2|\n",
      "|<brr>         |     1|     0|\n",
      "|<parenthood>  |     0|     1|\n",
      "|<b>           |     1|    26|\n",
      "|<ummm>        |     0|     1|\n",
      "|<demon>       |     0|     1|\n",
      "|<haha>        |     0|     1|\n",
      "|<gardenstuff> |     2|     0|\n",
      "|<weeping>     |     0|     1|\n",
      "|</cfoutput>   |     0|     1|\n",
      "|<g>           |     1|     0|\n",
      "|<twinkle>     |     0|     1|\n",
      "|<likewise>    |     1|     0|\n",
      "|<grin>        |     2|     0|\n",
      "|</summary>    |     1|     0|\n",
      "|<popcorn>     |     1|     0|\n",
      "|</joke>       |     1|     0|\n",
      "|<calc>        |     1|     0|\n",
      "|<justkiddin>  |     0|     1|\n",
      "|<atomic>      |     0|     1|\n",
      "|<p>           |     0|    16|\n",
      "|<strong>      |     0|     6|\n",
      "|<ducks>       |     1|     0|\n",
      "|</b>          |     1|    19|\n",
      "|<i>           |     0|    10|\n",
      "|<name>        |     0|     1|\n",
      "|</details>    |     1|     0|\n",
      "|</a>          |     1|     7|\n",
      "|</em>         |     0|     2|\n",
      "|<ht>          |     0|     1|\n",
      "|<br>          |     1|     6|\n",
      "|<ducking>     |     1|     0|\n",
      "|<attention>   |     0|     2|\n",
      "|<del>         |     0|     1|\n",
      "|<time>        |     1|     0|\n",
      "|<url>         | 98886|427976|\n",
      "|<thing>       |     0|     1|\n",
      "|<hugs>        |     1|     1|\n",
      "|<o>           |     0|     1|\n",
      "|<understood>  |     1|     0|\n",
      "|<grunt>       |     1|     0|\n",
      "|<retweet>     |     0|     1|\n",
      "|<outstanding> |     1|     0|\n",
      "|<agent>       |     0|     3|\n",
      "|<iostream>    |     0|     1|\n",
      "|<laugh>       |     1|     0|\n",
      "|</body>       |     0|     1|\n",
      "|<sigh>        |     0|     3|\n",
      "|<update>      |     0|     2|\n",
      "|<waves>       |     1|     0|\n",
      "|<w>           |     0|     1|\n",
      "|<moan>        |     0|     1|\n",
      "|<script>      |     0|     2|\n",
      "|</span>       |     0|     1|\n",
      "|<cfoutput>    |     0|     1|\n",
      "|</strong>     |     0|     6|\n",
      "|</html>       |     0|     1|\n",
      "|</div>        |     0|     1|\n",
      "|<cont>        |     1|     0|\n",
      "\n",
      "POS tweets contain 1,126,137 (11.88%) HTML tags.\n",
      "NEG tweets contain 1,006,539 (-10.62%) HTML tags.\n",
      "\n",
      " Frequency of tags\n",
      "Total: 5.419% (2,132,676 tags for 39,354,956 words)\n",
      "POS: 6.282% (1,126,137 tags for 17,926,681 words)\n",
      "NEG: 4.697% (1,006,539 tags for 21,428,275 words)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "## We check if the tags are relevant information between both pos and neg cases\n",
    "\n",
    "def count_HTML_tags(series) :\n",
    "    \"\"\"\n",
    "    Returns stats about the HTML tags in the tweet series.\n",
    "    Returns :\n",
    "    dic (defaultdict) : dict of all tags occurences.\n",
    "    count (int) : count of all tags.\"\"\"\n",
    "    \n",
    "    # Create counter for tags\n",
    "    dic = defaultdict(lambda:0)\n",
    "    # Create counting function\n",
    "    def a(k):\n",
    "        dic[k]+=1\n",
    "        return None\n",
    "    # Counts all encountered tags per tag\n",
    "    series.apply(lambda s : [a(k) for k in re.findall('<\\/*[a-zA-Z]+>', s)])\n",
    "    # Counts all tags in series\n",
    "    count = series.str.count('<\\/*[a-zA-Z]+>').sum()\n",
    "    return dic, count\n",
    "\n",
    "def count_words(series) :\n",
    "    \"\"\"\n",
    "    Counts the total number of words in a series of tweets.\n",
    "    \"\"\"\n",
    "    return series.str.split().apply(len).sum()\n",
    "\n",
    "# separate pos and neg datasets\n",
    "pos, neg = tweets.loc[tweets['label']==1], tweets.loc[tweets['label']==-1]\n",
    "\n",
    "# We query stats about the tags\n",
    "d_pos, n_pos = count_HTML_tags(pos['tweet'])\n",
    "d_neg, n_neg = count_HTML_tags(neg['tweet'])\n",
    "all_keys = set(d_pos.keys()) | set(d_neg.keys())\n",
    "\n",
    "print(f\"|{'KEY':14s}|{'POS':6s}|{'NEG':6s}|\")\n",
    "for k in all_keys : \n",
    "    print(f\"|{k:14s}|{d_pos[k]:6d}|{d_neg[k]:6d}|\")\n",
    "\n",
    "print(f\"\\nPOS tweets contain {n_pos:,} ({(n_pos-n_neg)*100/n_neg:.2f}%) HTML tags.\")\n",
    "print(f\"NEG tweets contain {n_neg:,} ({(n_neg-n_pos)*100/n_pos:.2f}%) HTML tags.\")\n",
    "\n",
    "nw_pos, nw_neg = count_words(pos['tweet']), count_words(neg['tweet'])\n",
    "nw_total = nw_pos+nw_neg\n",
    "\n",
    "print(f\"\\n Frequency of tags\")\n",
    "print(f\"Total: {(n_pos+n_neg)*100/nw_total:.3f}% ({n_pos+n_neg:,} tags for {nw_total:,} words)\")\n",
    "print(f\"POS: {(n_pos)*100/nw_pos:.3f}% ({n_pos:,} tags for {nw_pos:,} words)\")\n",
    "print(f\"NEG: {(n_neg)*100/nw_neg:.3f}% ({n_neg:,} tags for {nw_neg:,} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Although the difference in number of tags is not significant. The distribution of them is quite significant (i.e. for tags `<url>` and `<user>`). Thus we choose to leave the tags as part of the tweet. **THIS COULD BE REVIEWED TO IMPROVE PERF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the HTML tags from the tweets\n",
    "## CHANGE RETURN VAR IF RELEVANT\n",
    "\n",
    "def clean_HTML_tags(series) :\n",
    "    return series.str.replace('<\\/*[a-zA-Z]+>', '', regex=True)\n",
    "\n",
    "t = clean_HTML_tags(pos['tweet'])\n",
    "t2 = clean_HTML_tags(neg['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "In this part we train the models on our data.\n",
    "Thus, we perform\n",
    "* a resampling of our data to work locally on a smaller set.\n",
    "* the creation of word vectors for our tweets.\n",
    "* a train-test-split to locally estimate the model's performance.\n",
    "* cross-validation trainin on a series of models :\n",
    "    * Linear Regression\n",
    "    * Logistic Regression\n",
    "    * SVM\n",
    "    * Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the Training set\n",
    "Using only a set of 200'000 tweets locally to decrease computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>706939</th>\n",
       "      <td>me and my twin kind of dressed alike</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658792</th>\n",
       "      <td>\" &lt;user&gt; &lt;user&gt; aww thanks \" welcome</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229186</th>\n",
       "      <td>&lt;user&gt; don't worry , i'm stupid too</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11931</th>\n",
       "      <td>&lt;user&gt; hey , your amazing wish i could meet yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711440</th>\n",
       "      <td>&lt;user&gt; follow back please , laura ? and can yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243991</th>\n",
       "      <td>im gonna miss &lt;user&gt; when she leaves me</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967999</th>\n",
       "      <td>&lt;user&gt; nothing , she left me</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174022</th>\n",
       "      <td>i don't wanna go to school tomorrow</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>&lt;user&gt; i miss dressing up and running around n...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194721</th>\n",
       "      <td>- 046 viton o-ring , 75a durometer , black , 4...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet  label\n",
       "706939                me and my twin kind of dressed alike      1\n",
       "658792                \" <user> <user> aww thanks \" welcome      1\n",
       "1229186                <user> don't worry , i'm stupid too      1\n",
       "11931    <user> hey , your amazing wish i could meet yo...      1\n",
       "711440   <user> follow back please , laura ? and can yo...      1\n",
       "...                                                    ...    ...\n",
       "1243991            im gonna miss <user> when she leaves me     -1\n",
       "967999                        <user> nothing , she left me     -1\n",
       "1174022                i don't wanna go to school tomorrow     -1\n",
       "2724     <user> i miss dressing up and running around n...     -1\n",
       "1194721  - 046 viton o-ring , 75a durometer , black , 4...     -1\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Take only `0.5*local_t_size` samples from both classes for faster computation\n",
    "n = int(local_t_size/2)\n",
    "pos_ = resample(pos, n_samples=n, replace=False)\n",
    "neg_ = resample(neg, n_samples=n, replace=False)\n",
    "tweets_ = pos_.append(neg_)\n",
    "\n",
    "tweets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for 101,296.0 with 100 features for each word.\n",
      "Embeddings shape : (101296, 100).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt;</th>\n",
       "      <td>0.021177</td>\n",
       "      <td>0.007691</td>\n",
       "      <td>-0.014940</td>\n",
       "      <td>-0.026847</td>\n",
       "      <td>-0.000757</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>0.007588</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>-0.022654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011922</td>\n",
       "      <td>-0.012886</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.008529</td>\n",
       "      <td>-0.036729</td>\n",
       "      <td>0.021993</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>-0.015566</td>\n",
       "      <td>0.018960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.007715</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>-0.005271</td>\n",
       "      <td>-0.017900</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>-0.001585</td>\n",
       "      <td>-0.011057</td>\n",
       "      <td>-0.018982</td>\n",
       "      <td>-0.012275</td>\n",
       "      <td>-0.038192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000564</td>\n",
       "      <td>-0.019415</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>-0.024587</td>\n",
       "      <td>0.027616</td>\n",
       "      <td>-0.025369</td>\n",
       "      <td>0.035884</td>\n",
       "      <td>0.031967</td>\n",
       "      <td>-0.024375</td>\n",
       "      <td>-0.002844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "<user>  0.021177  0.007691 -0.014940 -0.026847 -0.000757  0.007441  0.018871   \n",
       "!      -0.007715  0.007642 -0.005271 -0.017900  0.013720 -0.001585 -0.011057   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "<user>  0.007588  0.002171 -0.022654  ... -0.011922 -0.012886  0.007829   \n",
       "!      -0.018982 -0.012275 -0.038192  ... -0.000564 -0.019415  0.003203   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "<user>  0.000143 -0.008529 -0.036729  0.021993  0.005002 -0.015566  0.018960  \n",
       "!      -0.024587  0.027616 -0.025369  0.035884  0.031967 -0.024375 -0.002844  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall our data\n",
    "print(f\"Embeddings for {embeddings.shape[0]:,.1f} with {embeddings.shape[1]} features for each word.\") \n",
    "print(f'Embeddings shape : {embeddings.shape}.\\n')\n",
    "embeddings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tweet, bagofwords=False):\n",
    "    \"\"\"\n",
    "    Creates the feature vector corresponding to the tweet.\n",
    "    To do so, computes the mean of the word embeddings corresponding to the vocabulary words in the tweet.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    tweet : str\n",
    "        Input tweet from which the word vector is created.\n",
    "    \"\"\"\n",
    "    split_by_words = tweet.split()\n",
    "    embed_list = []\n",
    "    \n",
    "    # Get vocab word embeddings\n",
    "    for w in split_by_words:\n",
    "        if w in words.index :\n",
    "            embed_list.append(  embeddings.loc[w].values  )\n",
    "        \n",
    "    # Compute mean if any vocab word was found\n",
    "    result = embed_list\n",
    "    if bagofwords :\n",
    "        if len(result) > 0 :\n",
    "            max_length = np.amax([len(x) for x in result], axis=0)\n",
    "            result = [np.pad(x,pad_width=(max_length-len(x)) ).tolist() for x in result]\n",
    "    else :\n",
    "        result = np.zeros(embeddings_dim_info) if not embed_list else np.mean(result, axis=0) \n",
    "        result = result.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import cpu_count, Pool\n",
    " \n",
    "cores = cpu_count() #Number of CPU cores on your system\n",
    "partitions = cores #Define as many partitions as you want\n",
    " \n",
    "def parallelize(data, func):\n",
    "    \"\"\"\n",
    "    Uses all CPU cores available to compute the function on each element of the data.\n",
    "    \"\"\"\n",
    "    print(f\"Computing function on {cores} cores.\")\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    print(data_split)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "import ctypes, os\n",
    "# Checking if the process is executed as admin (multiprocessing does not work otherwise) \n",
    "def isAdmin():\n",
    "    \"\"\"\n",
    "    Verifies if the process is executed as administrator.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        is_admin = (os.getuid() == 0)\n",
    "    except AttributeError:\n",
    "        is_admin = ctypes.windll.shell32.IsUserAnAdmin() != 0\n",
    "    return is_admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.errors import MergeError\n",
    "\n",
    "def get_word_vectors(tweets_, embedding_method = 'mean', recompute=False): \n",
    "    tweets_ = tweets_.copy()\n",
    "    try :\n",
    "        if recompute :\n",
    "            raise FileNotFoundError()\n",
    "        \n",
    "        # Load pre-computed word vectors file if possible \n",
    "        name = precomputed_file_location+'word_vectors_'+str(local_t_size)+'_'+str(embeddings_dim_info)+'.npy'\n",
    "        print(f\"Trying to load word vectors from {name}\")\n",
    "        precomputed = None\n",
    "        if divided_word_vectors :\n",
    "            print('Loading from divided dataset...')\n",
    "            precomputed = concatenate(word_vectors_files)\n",
    "        else : \n",
    "            precomputed = np.load(name, allow_pickle=True)\n",
    "\n",
    "        # Transform into Dataframe for merge\n",
    "        precomputed = pd.DataFrame(precomputed, columns=['index', 'label', 'mean_embed'])\n",
    "        pos_ = pos.loc[precomputed.loc[precomputed['label']==1, 'index']]\n",
    "        neg_ = neg.loc[precomputed.loc[precomputed['label']==-1, 'index']]\n",
    "        tweets_ = pos_.append(neg_).reset_index()\n",
    "        tweets_ = tweets_.merge(precomputed, how='inner', on=['index', 'label'], validate='1:1').set_index('index')\n",
    "        print('Successfully loaded from file!')\n",
    "\n",
    "    except (FileNotFoundError, MergeError) as e :\n",
    "        # Create word vectors for the local dataset\n",
    "        print('Could not load word vectors from file...\\nRecomputing word vectors...')\n",
    "        \n",
    "        func = None\n",
    "        if embedding_method=='mean':\n",
    "            func = word_vector\n",
    "        else : \n",
    "            func = lambda t : word_vector(t, bagofwords=True) \n",
    "        \n",
    "        if isAdmin():\n",
    "            print('Process is run as admin. Running parallelized computation...')\n",
    "            tweets_['mean_embed']= parallelize(tweets_['tweet'], func)\n",
    "        else : \n",
    "            print('Process is not run as admin. Cannot run parallelized setting, running as sequential...')\n",
    "            tweets_['mean_embed']= tweets_['tweet'].map(func)\n",
    "            \n",
    "    return tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load word vectors from ../precomputed_data/word_vectors_100000_100.npy\n",
      "Loading from divided dataset...\n",
      "Successfully loaded from file!\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%time tweets_ = get_word_vectors(tweets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>mean_embed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1193381</th>\n",
       "      <td>i cannot wait to party with &lt;user&gt; tonight</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.032002123948292126, -0.014642657203437489, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644929</th>\n",
       "      <td>haha she talking shit but don't know me . gosh...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.10238272539678156, -0.02736877932503185, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578303</th>\n",
       "      <td>&lt;user&gt; excellent , as long as he keeps a bit o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.032941437860502285, 0.0014634508830161917, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525309</th>\n",
       "      <td>&lt;user&gt; i'll be in mi on sunday ... i want to c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04879550502681794, 0.017707842326910788, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34941</th>\n",
       "      <td>getting told you look like the person that dri...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1420258276670681, -0.03843481256309387, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159713</th>\n",
       "      <td>&lt;user&gt; hey ! ! what time's your recital ? turn...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.15079636217861106, 0.005626630631020118, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249113</th>\n",
       "      <td>i miss being a kid . no one cared how you dres...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.05451928253254213, -0.052474023026710545, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913236</th>\n",
       "      <td>just cannot understand how to work tumblr prop...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.09112505455684311, 0.040824608634503316, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830941</th>\n",
       "      <td>the trail of tears ( events that shaped americ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.12102069953544096, -0.047875157832210066, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34871</th>\n",
       "      <td>received an mms from maxis . ' my heart is loc...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.12092917762249158, -0.06853298903401578, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet label  \\\n",
       "index                                                              \n",
       "1193381         i cannot wait to party with <user> tonight     1   \n",
       "644929   haha she talking shit but don't know me . gosh...     1   \n",
       "578303   <user> excellent , as long as he keeps a bit o...     1   \n",
       "525309   <user> i'll be in mi on sunday ... i want to c...     1   \n",
       "34941    getting told you look like the person that dri...     1   \n",
       "...                                                    ...   ...   \n",
       "1159713  <user> hey ! ! what time's your recital ? turn...    -1   \n",
       "249113   i miss being a kid . no one cared how you dres...    -1   \n",
       "913236   just cannot understand how to work tumblr prop...    -1   \n",
       "830941   the trail of tears ( events that shaped americ...    -1   \n",
       "34871    received an mms from maxis . ' my heart is loc...    -1   \n",
       "\n",
       "                                                mean_embed  \n",
       "index                                                       \n",
       "1193381  [0.032002123948292126, -0.014642657203437489, ...  \n",
       "644929   [0.10238272539678156, -0.02736877932503185, 0....  \n",
       "578303   [0.032941437860502285, 0.0014634508830161917, ...  \n",
       "525309   [0.04879550502681794, 0.017707842326910788, 0....  \n",
       "34941    [0.1420258276670681, -0.03843481256309387, 0.0...  \n",
       "...                                                    ...  \n",
       "1159713  [0.15079636217861106, 0.005626630631020118, 0....  \n",
       "249113   [0.05451928253254213, -0.052474023026710545, 0...  \n",
       "913236   [0.09112505455684311, 0.040824608634503316, 0....  \n",
       "830941   [0.12102069953544096, -0.047875157832210066, 0...  \n",
       "34871    [0.12092917762249158, -0.06853298903401578, 0....  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word vectors file to gain time\n",
    "def save_word_vectors(tweets_, ending=None):\n",
    "    word_vectors = tweets_[['label', 'mean_embed']].reset_index()\n",
    "    name = precomputed_file_location+'word_vectors_'+str(local_t_size)+'_'+str(embeddings_dim_info)\n",
    "    if ending is not None :\n",
    "        name = name+'_'+str(ending)\n",
    "    np.save(name, word_vectors)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_word_vectors(tweets_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split for our local dataset\n",
    "We divide our local training set into a 75% training set and a 25% local testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(tweets_, test_size=0.25) : \n",
    "\n",
    "    # train-test split\n",
    "    train_, test_ = train_test_split(tweets_, test_size=0.25)\n",
    "    print(f\"Local training set size : {train_.shape}.\")\n",
    "    print(f\"Local testing set size : {test_.shape}.\\n\")\n",
    "     \n",
    "    # Create features and label datasets\n",
    "    xtrain_, ytrain_ = train_.mean_embed.copy().tolist(), train_.label.copy().to_list()\n",
    "    xtest_, ytest_ = test_.mean_embed.copy().tolist(), test_.label.copy().tolist()\n",
    "\n",
    "    print('Training set first sample:\\n', xtrain_[0])\n",
    "    \n",
    "    return xtrain_, ytrain_, xtest_, ytest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local training set size : (75000, 3).\n",
      "Local testing set size : (25000, 3).\n",
      "\n",
      "Training set first sample:\n",
      " [0.08491972586172956, -0.0006829616033511412, 0.14126465341432295, -0.09241455557147843, 0.04876808797529943, -0.16686223489592514, 0.035675272561374295, -0.2940913667591907, 0.033400454715225576, -0.034256371554909355, -0.14889582795081657, -0.08566275121706629, 0.04030775117066337, -0.15073645027200966, 0.07866332548836048, 0.07052218479412098, 0.1982001088676559, -0.11791526770575726, 0.06011569947684016, 0.08166355691160566, -0.014641863636874548, -0.12742676087273666, 0.12495137724524116, 0.1588602299291537, -0.12025498151069416, -0.03804849935170046, -0.09563144564382334, -0.02009424276570287, 0.09168130082138959, -0.034246626888365383, 0.006922452892200677, 0.03741550447137739, -0.06207105838687338, 0.12388066292387201, 0.21328854159803803, -0.006238835032407843, 0.09674921125188798, 0.13512442005890177, -0.14928941057241854, 0.10771956712659773, 0.14493562273406704, 0.03311273833463225, 0.19690881043728692, 0.0026133719412841762, 0.19997853065447288, -0.20534524764135434, 0.04079857261900645, 0.008459550827175605, 0.02844578621229466, 0.2542709192450819, 0.24960373733900768, -0.05445021394367744, -0.08281169102784504, -0.2758266898606678, -0.0561116133225196, -0.012139884446596761, -0.0809558676968263, 0.03795048492996452, 0.10549607055258185, -0.08616946729110297, -0.044200224925707216, 0.08609185662939006, 0.12418704974060715, 0.01603148123005555, -0.23020128328997402, 0.006546819177103769, -0.011627019683674144, -0.000554007956584628, -0.04870793447252471, 0.1028017697094182, -0.05125045286864263, -0.11585625052623572, -0.06638523719015625, -0.10018391980981152, -0.03393745309712994, -0.09273064608957579, -0.18291018606747136, -0.09766500416026946, -0.004450056151406949, -0.13461962079914677, -0.0590565174182495, -0.16664307434550873, 0.016798164685291, -0.0655222801854907, -0.03142699802503861, -0.09198087104181582, -0.03166094337745829, -0.08942265791050355, -0.0911254231610095, -0.058290173706970067, -0.017368775924915307, -0.10510733538606973, 0.11093318042417097, -0.11048080075031484, 0.16544063937991926, -0.11046656801756176, 0.16028762683763037, -0.02056077356049198, 0.011052754633488492, -0.12645195494667985]\n"
     ]
    }
   ],
   "source": [
    "xtrain_, ytrain_, xtest_, ytest_ = prepare_data(tweets_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Here we compute our pre-processing on features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "def preprocess(X) :\n",
    "    x=X.copy()\n",
    "    \n",
    "    # Standardize data\n",
    "    standardizer=StandardScaler().fit_transform(x)\n",
    "    \n",
    "    # TODO Polynomial features and interactions\n",
    "    \n",
    "    # other data preprocessing\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process training set\n",
    "\n",
    "xtrain_ = preprocess(xtrain_)\n",
    "xtest_ = preprocess(xtest_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating efficiency of model\n",
    "Here we define metrics for model classification efficiency.\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for Linear Model is 0.0528607913232263.\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Linear Regression\n",
    "name = 'Linear Model'\n",
    "\n",
    "linear_classifier = LinearRegression().fit(xtrain_, ytrain_)\n",
    "score = linear_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (linear_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for Logistic Regression is 0.59272.\n",
      "R2 score for Logistic Regression model using cross-validation is 0.5928.\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Logistic Regression\n",
    "name = 'Logistic Regression'\n",
    "\n",
    "logistic_classifier = LogisticRegression().fit(xtrain_, ytrain_)\n",
    "score = logistic_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (logistic_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} is {score}.\")\n",
    "\n",
    "# Logistic Regression using Crossvalidation\n",
    "name = 'Logistic Regression using cross-validation'\n",
    "\n",
    "logisticCV_classifier = LogisticRegressionCV().fit(xtrain_, ytrain_)\n",
    "score = logisticCV_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (logisticCV_classifier, score)\n",
    "\n",
    "print(f\"R2 score for Logistic Regression model using cross-validation is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos    0.57472   0.70490   0.63319     12467\n",
      "         neg    0.62107   0.48113   0.54222     12533\n",
      "\n",
      "    accuracy                        0.59272     25000\n",
      "   macro avg    0.59790   0.59302   0.58770     25000\n",
      "weighted avg    0.59796   0.59272   0.58758     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "predtest_ = logistic_classifier.predict(xtest_)\n",
    "\n",
    "report = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos    0.57441   0.70803   0.63426     12467\n",
      "         neg    0.62213   0.47818   0.54074     12533\n",
      "\n",
      "    accuracy                        0.59280     25000\n",
      "   macro avg    0.59827   0.59310   0.58750     25000\n",
      "weighted avg    0.59834   0.59280   0.58738     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "predtest_ = logisticCV_classifier.predict(xtest_)\n",
    "\n",
    "report = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for SVM classifier model is 0.59404.\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Support Vector Machines\n",
    "name = 'SVM classifier'\n",
    "\n",
    "SVM_classifier = LinearSVC().fit(xtrain_, ytrain_)\n",
    "score = SVM_classifier.score(xtest_, ytest_)\n",
    "\n",
    "classifiers[name] = (SVM_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} model is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for Neural Network classifier is 0.608.\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### BASELINE : Neural Networks\n",
    "\n",
    "# Neural Network\n",
    "name = 'Neural Network'\n",
    "\n",
    "nn_classifier = MLPClassifier().fit(xtrain_,ytrain_)\n",
    "score = nn_classifier.score(xtest_,ytest_)\n",
    "\n",
    "classifiers[name] = (nn_classifier, score)\n",
    "\n",
    "print(f\"R2 score for {name} classifier is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos    0.59458   0.67380   0.63172     12474\n",
      "         neg    0.62546   0.54247   0.58102     12526\n",
      "\n",
      "    accuracy                        0.60800     25000\n",
      "   macro avg    0.61002   0.60814   0.60637     25000\n",
      "weighted avg    0.61005   0.60800   0.60631     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "predtest_ = nn_classifier.predict(xtest_)\n",
    "\n",
    "metrics = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5, output_dict=True)\n",
    "report = classification_report(ytest_, predtest_, labels=[1,-1], target_names=['pos', 'neg'], digits=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline results\n",
    "path = '../results/'\n",
    "name = path+'metrics_'+str(local_t_size)+'_'+str(embeddings_dim_info)+'_baseline'\n",
    "np.save(name, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom models\n",
    "Let us implement a custom model following the paper $\\text{Text Classification with Deep Neural Networks}$ by *Maaz Amajd et al.* from Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recompute word vectors datasets\n",
    "The convolutionnal Neural Network uses a matrix as input format. Here we use a matrix of words vectors representing the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load word vectors from file...\n",
      "Recomputing word vectors...\n",
      "Process is not run as admin. Cannot run parallelized setting, running as sequential...\n"
     ]
    }
   ],
   "source": [
    "tweets_cnn = get_word_vectors(tweets_, embedding_method='BOW', recompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>mean_embed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1193381</th>\n",
       "      <td>i cannot wait to party with &lt;user&gt; tonight</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.006390639092777149, -0.002517977760506535,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644929</th>\n",
       "      <td>haha she talking shit but don't know me . gosh...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.095365011208915, -0.002271404137185445, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578303</th>\n",
       "      <td>&lt;user&gt; excellent , as long as he keeps a bit o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.021176976452678558, 0.007691384674272959, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525309</th>\n",
       "      <td>&lt;user&gt; i'll be in mi on sunday ... i want to c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.021176976452678558, 0.007691384674272959, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34941</th>\n",
       "      <td>getting told you look like the person that dri...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.07722609727291953, -0.015988683796022186, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159713</th>\n",
       "      <td>&lt;user&gt; hey ! ! what time's your recital ? turn...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.021176976452678558, 0.007691384674272959, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249113</th>\n",
       "      <td>i miss being a kid . no one cared how you dres...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.006390639092777149, -0.002517977760506535,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913236</th>\n",
       "      <td>just cannot understand how to work tumblr prop...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.06250523629347589, -0.01246092651515487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830941</th>\n",
       "      <td>the trail of tears ( events that shaped americ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-0.002497237240290465, 0.0335195033127824, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34871</th>\n",
       "      <td>received an mms from maxis . ' my heart is loc...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.7619069241897088, 0.2361159544230768, -0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet label  \\\n",
       "index                                                              \n",
       "1193381         i cannot wait to party with <user> tonight     1   \n",
       "644929   haha she talking shit but don't know me . gosh...     1   \n",
       "578303   <user> excellent , as long as he keeps a bit o...     1   \n",
       "525309   <user> i'll be in mi on sunday ... i want to c...     1   \n",
       "34941    getting told you look like the person that dri...     1   \n",
       "...                                                    ...   ...   \n",
       "1159713  <user> hey ! ! what time's your recital ? turn...    -1   \n",
       "249113   i miss being a kid . no one cared how you dres...    -1   \n",
       "913236   just cannot understand how to work tumblr prop...    -1   \n",
       "830941   the trail of tears ( events that shaped americ...    -1   \n",
       "34871    received an mms from maxis . ' my heart is loc...    -1   \n",
       "\n",
       "                                                mean_embed  \n",
       "index                                                       \n",
       "1193381  [[0.006390639092777149, -0.002517977760506535,...  \n",
       "644929   [[0.095365011208915, -0.002271404137185445, 0....  \n",
       "578303   [[0.021176976452678558, 0.007691384674272959, ...  \n",
       "525309   [[0.021176976452678558, 0.007691384674272959, ...  \n",
       "34941    [[0.07722609727291953, -0.015988683796022186, ...  \n",
       "...                                                    ...  \n",
       "1159713  [[0.021176976452678558, 0.007691384674272959, ...  \n",
       "249113   [[0.006390639092777149, -0.002517977760506535,...  \n",
       "913236   [[0.06250523629347589, -0.01246092651515487, 0...  \n",
       "830941   [[-0.002497237240290465, 0.0335195033127824, -...  \n",
       "34871    [[0.7619069241897088, 0.2361159544230768, -0.2...  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_word_vectors(tweets_cnn, ending='cnn')\n",
    "tweets_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local training set size : (75000, 3).\n",
      "Local testing set size : (25000, 3).\n",
      "\n",
      "Training set first sample:\n",
      " [[0.03874826280722202, -0.0226154273381748, 0.06331501800493958, -0.05300686099727286, -0.02604800050298649, -0.06584109852584337, -0.008828779663752315, -0.10206945408357572, -0.00353245552938762, -0.029851338185586506, -0.0332773668685895, -0.0772952293011356, -0.028125405154649213, -0.07623803516338871, 0.02133121024922275, 0.02422468594373341, 0.01366152352805924, -0.02749200397469116, 0.09053584800823926, -0.023833276378709092, 0.004043477932097031, -0.05621087884414685, 0.0526542836005664, 0.045542285624793066, -0.033528027208590556, -0.017383583133306613, -0.03443335386155496, -0.10966482189803363, 0.04206032044073065, -0.07445478974047048, 0.007670329872138221, 0.0509657715676167, -0.014440089971403428, 0.11515830005637248, 0.10204113604695303, -0.04486226197048761, 0.0791685780560366, 0.025574548862244122, -0.045750544964075976, 0.0499636137507949, 0.03762462946514737, 0.043730395505155895, 0.04042619453658016, 0.017827619406573566, 0.19176841335987221, -0.10167572891057736, 0.03661949815765191, -0.010080986086904997, -0.00043321925557569764, 0.023427650270941316, 0.07844131990371339, -0.07034712299576808, 0.03782711965822852, -0.09519614905896609, 0.009965385624857272, -0.03385306864796681, 0.028001230037181447, -0.05716964830084966, 0.10912491784440964, -0.044073634639243194, 0.009529818356313027, -0.01743463866427918, 0.04312266979171952, -0.005398493699931578, -0.07738709849223772, -0.06390053399426142, -0.018053386382628368, -0.02843255580277417, 0.011263346488290516, 0.011847996506057301, -0.11803265665910766, -0.08123417958207392, 0.013421684691729382, -0.13854675456210064, 0.036650607234384375, 0.017695919438092268, -0.06646973924601096, 0.053211119784353375, 0.013474056780384377, -0.04126979786365878, -0.00959953855138115, -0.1029459182552377, 0.014743141533963751, -0.020117150336572957, -0.010700014460789593, -0.07595102547034846, -0.0207070806318504, 0.022023765724119963, -0.017998417378203912, -0.011970413917000756, -0.09077017031471867, -0.08566726060697527, 0.14645275147005818, -0.14223984250592392, 0.05566548757570775, -0.027030644171744538, 0.06319442594210875, -0.06277219930595619, 0.020477970696876277, -0.027097211318444503], [0.05820608759052884, 0.0022495328327372787, 0.05592900231709098, -0.07272714351321583, -0.011139520771943238, -0.07197715599126948, 0.01824578946921668, -0.14154678959127096, -0.00961572511579202, -0.04654448666806063, -0.044890793351426435, -0.09964836474909314, -0.0214941979998027, -0.07894129783545852, 0.02164564763259233, 0.04289817113236922, 0.00631849209159897, -0.017618329146167976, 0.13358727311270682, -0.03825997471605937, 0.012023670981589319, -0.07877692346385555, 0.07251325417322471, 0.060973751361367026, -0.07236442834864838, -0.02302867804603923, -0.07603134167986773, -0.15629166587235424, 0.06970779318031717, -0.059988513430677355, -0.013560615080073285, 0.06370665105287864, 0.023091199483345634, 0.18497032655685888, 0.15244875328284055, -0.09027318972360529, 0.10245313005966954, 0.0535873181125109, -0.09659701096046538, 0.10783268541216598, 0.038790248908239254, 0.039744460666295015, 0.08900346384853654, 0.04249945595610479, 0.2794367731259615, -0.1710077670830055, 0.02724512724517111, -0.016658664272653938, 0.0011378770765326012, 0.04699576477543221, 0.13509798224906747, -0.06851684314536104, 0.03774614163037288, -0.1607475406541802, 0.009663668655153492, -0.061538855465913415, 0.03976635800594569, -0.1015704546278467, 0.1428398025490679, -0.07686275450925178, 0.020259780522727196, -0.04921360649107523, 0.04197825208437956, -0.01523737128419591, -0.07852210805132083, -0.07251412382274655, -0.04772270700061804, 0.004564681077092937, -0.0015970195958259846, 0.03261244235519234, -0.16772304044204112, -0.15134153065255146, 0.029968944332917507, -0.21935761920010377, 0.08336773641575977, 0.03239715524241938, -0.056717222463693574, 0.059080234635765706, 0.01584215095941949, -0.0844248378849269, -0.0028686903138194827, -0.12412097537784451, -0.022350418354654566, -0.02485676976163906, -0.0029115821461034685, -0.12095931811397967, -0.027121567560003543, 0.06530838983210088, -0.03349178702642274, -0.006951111641130268, -0.15598208557837057, -0.11790873983491176, 0.2195237103862889, -0.20751254986142428, 0.049924984732186366, -0.06340375744226763, 0.07229356218703047, -0.11771285891955358, 0.01805332206872844, -0.054492086282707144], [-0.19216937896752379, -0.18459381231553879, 0.15021902608889381, -0.15036323717040487, -0.23473951771613363, -0.15019933576268427, -0.027707246703501852, -0.35850633916351066, -0.043643958519660596, -0.08987065385048998, 0.08168295882905287, -0.1109036429055195, 0.2377179688944711, -0.06899924889212158, 0.22946386297252824, -0.15740330925828952, 0.16133105521039842, -0.0899297437185177, 0.08397439551877403, 0.03849267075430831, 0.006682631054480497, -0.2244923596571173, -0.04350558044305328, 0.034697818881145356, -0.20160148838473133, -0.13141442522649976, -0.2731807824675881, -0.16727427471231257, 0.1689869989518395, -0.13019254100150024, 0.23027326470788537, 0.23973484813077459, -0.13163571385401163, 0.7331070019572344, 0.25449493737720563, -0.06276526187685744, 0.09246462396615954, 0.06448362768893984, -0.3456522703605619, 0.09371686998972005, -0.13202724434786234, 0.027481924320620386, 0.030483154043455658, 0.1057387954362006, 0.6051127211843214, -0.39886431568519004, -0.1547312815814569, -0.07596103828012493, 0.21203332627721272, 0.02835721157776318, 0.1826751960966946, 0.12952593624328662, 0.23335543728848177, -0.2982319420557599, 0.1310649265573444, -0.17729141195799886, 0.10796266375664929, -0.2357612775405139, -0.07813163791339375, -0.05846707707131342, -0.03242320266628979, -0.32843927854938504, 0.2924589003074826, -0.14911148707414143, -0.11649231923600002, 0.08840137981938635, -0.20292660532102605, 0.0005167189763296708, 0.08681768349547866, 0.14405718443954774, -0.46087998191549884, -0.35055590058146535, -0.179967083290669, -0.3652962362823989, 0.10904926360091603, 0.06775759848869775, -0.2134168531414477, 0.057733452436219454, -0.0677871042262281, -0.1262399758761953, -0.23209938045935735, -0.2911171915012088, 0.12765461575578588, 0.09501104239291536, -0.13984976014494635, -0.19174593746116045, 0.1316784475458429, -0.1992135178569957, -0.1467788498688385, -0.10377572468814714, -0.1966713272331518, -0.11179973541521511, 0.056226028731285675, -0.3159408473868224, -0.01937181933698707, -0.24896044875699636, 0.011853059469522947, -0.16382287260879178, 0.053336293192484924, -0.10739986070856991], [0.0027545743641906144, 0.0052100503881193794, 0.0026322899284604057, 0.0035652504056979863, 0.060147497836464066, -0.015125914185375865, 0.0060059212489176495, -0.009220229770744649, 0.0039360493991093615, -0.0016001279475589352, -0.006781617371801093, 0.04832437587494029, 7.880960000203885e-05, -0.003243144469023905, 0.017678517956893697, 0.002484997176322467, 0.01661562625545125, -0.006532931797656542, 0.00455803772731901, 0.03830456391135628, 0.007634838950340619, 0.003577651815269952, -0.008058567167417097, -0.020774347087562612, -0.013242112282120673, -0.030354361250925967, -0.04247067447842525, -0.015296551174782905, 0.04163414468959251, -0.010455948217622671, 0.012134500353266587, -0.006352530178640601, -0.04341526316912584, 0.02400173612692962, 0.0205661973367328, 0.025076495497212473, -0.030179661238226557, 0.004658497762236279, -0.027115285206692194, 0.02328340782122058, 0.014069077170833784, -0.021187441359492198, -0.0037566804804181012, -0.0032951404148482345, 0.033298770177679224, 0.0210194676655666, -0.0007275164355316272, 0.003916440792351968, 0.0014695411763435322, -0.019807136891309713, -0.010160279090746044, 0.0003567272797723158, 0.06185841820193822, 0.016043446489968475, 0.042723530825558534, -0.0009514098177313903, 0.02369010952896426, -0.013249428407011016, 0.015366909766182138, -0.01510852795017399, 0.006275362371251281, 0.039008180820299665, -0.0275175425505643, 0.01229724462332223, -0.026962389267507037, 0.026144073249363915, -0.022907872346236122, -0.01592486846567347, 0.01167292965700065, 0.02049593387189821, 0.006823601103504785, -0.022794209911539846, 0.007079818480414288, 0.0022822206138110304, 0.0004197265363002411, 0.005605468383396001, -0.026675561984632837, 0.03390913322950142, -0.004672655134847359, -0.0179084458304655, -0.032255748622730965, 0.012086025813572158, -0.008232146307451392, -0.026699741077308367, 0.0004902798547477128, 0.003955720720034941, 0.022426956063241132, 0.0007790969622678728, 0.0031504755167318507, 0.0007554888404524806, -0.009073752284675229, -0.03061165418539486, -0.0013534472449101991, -0.02075989026297182, 0.01765257319583058, 0.042277956247533145, 0.0043655003936694465, 0.02477620639528626, -0.03013439267912033, 0.009534976941133233], [0.07515433184050824, -0.02924703695566412, 0.13354649385819214, -0.14519354490168107, -0.08009635938363278, -0.175190224796296, -0.0013535406959149336, -0.25458765032929503, -0.021503923901046326, -0.08179496543800516, -0.0816332301169702, -0.20164997451911118, -0.04855552278859872, -0.1771123293652343, 0.04008214009888767, 0.09909152481963737, 0.015813866818656687, -0.037713993300144216, 0.2497454712985588, -0.08415147516594096, 0.028195932501263912, -0.15919535656573855, 0.10738059167192957, 0.13542311793280243, -0.0742185998528798, -0.0348175386384035, -0.12163187475109939, -0.2688108834883237, 0.11131688146361553, -0.13447733565274403, -0.0028020129631008686, 0.11564654403515874, -0.01018498982212832, 0.36129406033750766, 0.2908383839384728, -0.1748474518813453, 0.200134008539612, 0.09179789217089111, -0.18522673005412937, 0.1572973652212067, 0.0778416213116179, 0.11738599934888744, 0.12704615286575802, 0.0525361968715133, 0.5286329838265903, -0.30238931546577225, 0.067259499541434, -0.042619284159097055, 0.011438450864488876, 0.08131161907671326, 0.25075061364392043, -0.10662083730623212, 0.06763996436603019, -0.2880780197875056, 0.04881645594082596, -0.09230435630792991, 0.03624801866647685, -0.13382957666942083, 0.24255346608589473, -0.10142349576097076, 0.05482952664679435, -0.06548941602467875, 0.12707105646037337, -0.02487226633512448, -0.18782692064410547, -0.12138910143106774, -0.09550845288575299, -0.012798845723220818, -0.03902109882307062, 0.08131361057783822, -0.285603593858939, -0.2174291645511087, 0.03507193688086718, -0.371069710897142, 0.16291978386682063, 0.07318975501195732, -0.1344472888410358, 0.11809333736038383, 0.0518470698114975, -0.1518363025414729, 0.027521430314695523, -0.250782350484579, -0.022131931967478643, -0.025881474057089765, -0.025679276761260014, -0.21816145783110966, -0.03856456976204739, 0.06649501983576563, -0.030052430661722935, -0.007960688187052412, -0.2337733342820218, -0.24231773790923417, 0.4194212128172572, -0.34608104699595627, 0.09742841909650903, -0.0616134979688927, 0.12840184780056604, -0.21306758179902024, 0.03109192999913905, -0.09863127447132354], [0.0691572792807175, -0.01312411951679047, 0.13980825118309378, -0.11186770811240329, -0.0605956913197638, -0.15540948632245896, -0.013862032113596203, -0.2478336241363276, -0.043760348431526874, -0.08713180673177463, -0.06114522664096007, -0.150113768597251, -0.05821713389652822, -0.14533117930980619, 0.06117725060671246, 0.11494250914963286, 0.003760783196208864, -0.04960770516027261, 0.20222142837538887, -0.07382959857669802, 0.026169674913899054, -0.15312512751808344, 0.12520470433204997, 0.115476272125863, -0.07251603233816889, -0.024307016358051648, -0.09179296418247435, -0.22022397444844227, 0.05885584331124946, -0.11534945970089835, -0.019161038983485524, 0.09255171514670799, -0.003690490823252037, 0.3104021193291385, 0.27053021054972565, -0.14066657255936638, 0.20768370585870136, 0.07117919860377206, -0.184577343629972, 0.16707879438809306, 0.03643113142088478, 0.12826695732220667, 0.09587743479374763, 0.02914636551400659, 0.49202219249715806, -0.28016887974956, 0.05848876565649934, -0.049291906919303366, 0.0026641416209873684, 0.08107055388950164, 0.2544917551355878, -0.09684555617467717, 0.08770072096153733, -0.2382169933165747, 0.02088551504516912, -0.08189262063872174, 0.03792250659856985, -0.15426787296208624, 0.27525439518543227, -0.1192251657072366, 0.07193045742793748, -0.06091230840302329, 0.0878200537203543, -0.01723978085377881, -0.1600115864309396, -0.11317436958745633, -0.09570478656532823, 0.02527229384311331, -0.018370297212873813, 0.042124080715243706, -0.260674935331846, -0.20905992571591325, 0.04529761858962358, -0.31725647973434157, 0.13488382686439998, 0.02552431950972449, -0.12279547082774225, 0.10702094439195677, 0.04152306903989739, -0.14189104301216277, 0.003925914256898108, -0.20864136149742857, -0.00784566674122313, -0.023692047317318582, 0.026020286228551126, -0.17395982180103295, -0.0029453170623644992, 0.07479381183741815, -0.03148392040474831, 0.014238975525189967, -0.23291790872667245, -0.18291581118475303, 0.34496081310295146, -0.3052190985778598, 0.08856010385119713, -0.043278552651283886, 0.11976446240003111, -0.19594650182122955, 0.03855609978056234, -0.049601271487643414], [0.1020566985999923, -0.006643052749302242, 0.12886555371782257, -0.1246477806248689, -0.08233098386589374, -0.17501299293283237, -0.0102689764133075, -0.2983172579780782, -0.02040236981203195, -0.08982749399580733, -0.09678860723766046, -0.2312725626495482, -0.10214588038933738, -0.17564662141382364, 0.08207473773394003, 0.07415257626038618, -0.00027792980141612953, -0.028892026414784975, 0.280921764925327, -0.0830237192242573, 0.01816744873539309, -0.1801963450421498, 0.14990500267771367, 0.13318930931310763, -0.10551009519616805, -0.04538893412310743, -0.09833183309807543, -0.28305074351332216, 0.1363066461755239, -0.1206897152398501, 0.026856164361743, 0.10149807861480732, -0.02306120039145482, 0.38016586407690006, 0.31703165253764454, -0.18568296433056866, 0.20789585948441794, 0.10777644691374506, -0.20031586944073929, 0.16424325803405876, 0.08344476846832989, 0.139804781882901, 0.09427555611452043, 0.05736064277165888, 0.5246164526851739, -0.3258671214555689, 0.05666357859194896, -0.07984526610210543, -0.009082962230222088, 0.10619611050692114, 0.2784471628415889, -0.09313977789648975, 0.12551120106556787, -0.30541449894822775, 0.0024132499728285774, -0.13556530307209308, 0.06149030867239727, -0.1605451106304157, 0.2768531569390484, -0.12833808043044054, 0.030869350922570878, -0.09498712937267563, 0.1583352389603186, -0.027987332954837973, -0.19493236405030911, -0.15672411997993793, -0.09113128795284348, 0.024472224982630464, -0.0488950308494302, 0.07945816120951997, -0.30469960844858957, -0.2283589390724489, 0.07258572975303082, -0.4193445504007812, 0.12150360789712006, 0.022448168786341514, -0.16288583223675604, 0.10436540471117627, 0.039268019643212033, -0.1705067832997982, 0.00295945419735046, -0.2951451460136482, -0.037581438975144967, -0.07220620133890092, -0.024012468459885427, -0.19680691657724556, -0.032120657156750936, 0.07492804412642952, -0.040204112020949964, 0.006368280603579507, -0.2584999722542277, -0.22424952562065031, 0.4374617426258517, -0.34228788590706855, 0.12117074749416404, -0.06346676463311088, 0.1617071740760636, -0.24609492818961332, 0.03661920209773757, -0.08697149591375485], [0.07436374983839178, -0.01905474268548291, 0.1287295608061761, -0.14517568960892055, -0.0609641796289088, -0.17411998812701157, 0.0014308455485270064, -0.2881914212498898, -0.029564060797347674, -0.08379444234507913, -0.07106732643351818, -0.16474914853344932, -0.06553352647473623, -0.1666260232867646, 0.036154636437664346, 0.08088942206128813, -0.009590457403524884, -0.034645466690874184, 0.24498421911471224, -0.09301173907363477, 0.04294546239781629, -0.16597473531133, 0.12110750424535538, 0.13720711313911405, -0.09820699403133422, -0.053886598975223854, -0.12305071154521291, -0.23983011910244492, 0.09909844676077793, -0.1605146398019212, 0.015480907964199946, 0.11729972881714394, -0.00959894930574319, 0.3486474211183854, 0.2742059304922802, -0.14782675181477611, 0.18431264662756586, 0.08363335399260709, -0.19129697991644148, 0.17762229662428622, 0.05257116680618082, 0.13453128571412115, 0.12142972781738523, 0.0408730027895796, 0.5038265154284864, -0.31160179693816914, 0.08935470786659128, -0.0712815613729251, 0.020834645235876047, 0.09563117745438847, 0.241501641832257, -0.11164365420221284, 0.08170126556102061, -0.2635914851531453, 0.061768830786005, -0.11862241876001176, 0.06296072956630143, -0.17200627106799798, 0.2641718111096616, -0.0988902282708519, 0.06061102717482227, -0.06695321700275939, 0.12161387437013318, -0.02172315300523183, -0.16751303986698662, -0.13487530470254913, -0.10032381808451857, 0.027059841502697836, -0.042071654497703566, 0.06606971578247917, -0.2937111308587792, -0.2253918074017661, 0.054335299944311376, -0.3448204367964576, 0.16012794006978656, 0.04590275850111483, -0.12903324112757844, 0.1265949675057719, 0.030850377909221995, -0.1428161738508605, 0.029228284310985904, -0.23916210699217227, -0.03513267722252979, -0.0337868208701595, -0.026980248887626273, -0.16348631145973197, -0.01975403989205712, 0.06538101386754905, -0.04070246200596251, -0.0013059177219584277, -0.25492325012054046, -0.2170908219936803, 0.4079396911159087, -0.33552267466404395, 0.10776384913680849, -0.06631093098517676, 0.13572636138830188, -0.21056665536035094, 0.02538433696240985, -0.07072058728245628], [0.07334670937383486, -0.009654711612605392, 0.12421537339038405, -0.11491016261683643, -0.06466623430213514, -0.1672374176929014, -0.03175249581954941, -0.2435428303715889, -0.006852770194524391, -0.11247588582860132, -0.08138399674390809, -0.20880150618744756, -0.08716029552615691, -0.20229922681829285, 0.07407229673592022, 0.1240687284862981, -0.012842009952032206, -0.069309308997094, 0.26274962575945376, -0.09756491376361412, 0.018213114607896663, -0.1831358733773425, 0.155138859132073, 0.14837049866190746, -0.09282620104717827, -0.09182426259114818, -0.08191174537009167, -0.2506024882019347, 0.13163193079739036, -0.13782337623537355, 0.000230741298244809, 0.08269668944388867, -0.033940187554852, 0.3644445771788002, 0.3180484281408125, -0.13071536905215075, 0.21346458504291704, 0.08004071912297321, -0.21972660849204456, 0.14474242853277883, 0.109551243674322, 0.11138756720322274, 0.11622285356049354, 0.05882104342504072, 0.5040295630092743, -0.3208957546580599, 0.0815043940801417, -0.05898679559412221, 0.008307753084080062, 0.11422261805303853, 0.26745441498209094, -0.11828774035711155, 0.09968951902465976, -0.26680658766597504, 0.03825594951741425, -0.14281862119843805, 0.03816729666964381, -0.16637698840052778, 0.268468860282916, -0.1451512235856671, 0.08788846851662617, -0.03173599167456553, 0.1579638762901177, 0.006730172130426026, -0.18658423943727212, -0.14079508069032678, -0.08479283222415598, 0.008064810001472578, -0.06809956145804914, 0.07724409000676258, -0.31077479857969703, -0.2462232497840166, 0.016934751275728528, -0.34814264465284867, 0.1330808458259868, 0.06536028278495862, -0.14784481904892458, 0.08085523843014383, 0.032310920661306954, -0.1802617472559888, 0.050449261295665404, -0.24262446149460995, -0.05814119758075909, -0.04238113675035681, -0.003343272050174639, -0.1594878059233816, -0.025659403985406035, 0.08901667432781366, -0.0414383971399819, 0.007442077684993643, -0.2663067945467369, -0.22905691985610876, 0.4421534056631639, -0.3113627727494416, 0.12821752115372242, -0.0569278811083221, 0.14054699498613005, -0.18990956081864382, 0.025654375024349942, -0.06062893272119833]]\n"
     ]
    }
   ],
   "source": [
    "xtrain_cnn, ytrain_cnn, xtest_cnn, ytest_cnn = prepare_data(tweets_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "# Define base parameters for the CNN\n",
    "\n",
    "batch_size = 64 ## TODO tune batch size (depends on mem)\n",
    "epochs = 20\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_, ytrain_ = np.array(xtrain_), np.array(ytrain_)\n",
    "xtest_, ytest_ = np.array(xtest_), np.array(ytest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_17 (Conv1D)           (None, 75000, 32)         6432      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 75000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 37500, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 37500, 64)         4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 37500, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 18750, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 18750, 128)        16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 18750, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 9375, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1200000)           0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               153600128 \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 153,627,490\n",
      "Trainable params: 153,627,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the layers of the CNN\n",
    "cnn_classifier = Sequential()\n",
    "cnn_classifier.add(Conv1D(32, kernel_size=2,activation='relu',input_shape=xtrain_.shape[] ,padding='same'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))\n",
    "cnn_classifier.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "cnn_classifier.add(Conv1D(64, 2, activation='relu',padding='same'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))\n",
    "cnn_classifier.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "cnn_classifier.add(Conv1D(128, kernel_size=2, activation='relu',padding='same'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))                  \n",
    "cnn_classifier.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "cnn_classifier.add(Flatten())\n",
    "cnn_classifier.add(Dense(128, activation='linear'))\n",
    "cnn_classifier.add(LeakyReLU(alpha=0.1))                  \n",
    "cnn_classifier.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "cnn_classifier.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "cnn_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 100]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\Nico\\.conda\\envs\\ada\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 100]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "cnn_history = cnn_classifier.fit(xtrain_, ytrain_, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(xtest_, ytest_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CNN results\n",
    "path = '../results/'\n",
    "name = path+'metrics_'+str(local_t_size)+'_'+str(embeddings_dim_info)+'_CNN'\n",
    "np.save(name, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "This section is dedicated to using the previous classifiers to predict the labels of the provided testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To format the testing data\n",
    "def extract_tweet(tweet):\n",
    "    return tweet.split(\",\", 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i cant stay away from bug thats my baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>had a nice time w / my friend lastnite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>&lt;user&gt; no it's not ! please stop !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>not without my daughter ( dvd two-time oscar (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>&lt;user&gt; have fun in class sweetcheeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>making a r . e . a . l . difference . ( get r ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "1      sea doo pro sea scooter ( sports with the port...\n",
       "2      <user> shucks well i work all week so now i ca...\n",
       "3                i cant stay away from bug thats my baby\n",
       "4      <user> no ma'am ! ! ! lol im perfectly fine an...\n",
       "5      whenever i fall asleep watching the tv , i alw...\n",
       "...                                                  ...\n",
       "9996              had a nice time w / my friend lastnite\n",
       "9997                  <user> no it's not ! please stop !\n",
       "9998   not without my daughter ( dvd two-time oscar (...\n",
       "9999                <user> have fun in class sweetcheeks\n",
       "10000  making a r . e . a . l . difference . ( get r ...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading data\n",
    "\n",
    "# Load the testing data\n",
    "test = pd.read_fwf(data_path+ 'test_data.txt', sep=\"\\n\", header=None)\n",
    "test = test.rename(columns={0:'tweet', 1:'na1', 2:'na2'})\n",
    "\n",
    "# Reformating it for submission\n",
    "test.index = test.index+1 # Format asked by AI Crowd\n",
    "test = test['tweet'].map(extract_tweet).to_frame()\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.14106374638176422,\n",
       "  -0.06872910829415042,\n",
       "  0.10606407952823267,\n",
       "  -0.09969504582330192,\n",
       "  -0.09938122724092968,\n",
       "  0.28317441988314734,\n",
       "  0.12138428352938964,\n",
       "  -0.1517520964155663,\n",
       "  0.10356461464215022,\n",
       "  -0.11447167273952473,\n",
       "  -0.1019050178288262,\n",
       "  0.04473441812472089,\n",
       "  0.12753700910123314,\n",
       "  -0.014779420598848767,\n",
       "  0.10883114641696463,\n",
       "  -0.027001854853501504,\n",
       "  0.10577392355516102,\n",
       "  -0.01237564492214693,\n",
       "  -0.08728130073213365,\n",
       "  -1.3278300604051836e-05,\n",
       "  0.001988548783500319,\n",
       "  0.09713995903112027,\n",
       "  0.15960496406624083,\n",
       "  0.03955682032454823,\n",
       "  0.06123988498368808,\n",
       "  0.017050798883699113,\n",
       "  0.021711058586347004,\n",
       "  0.02652509905392398,\n",
       "  0.07778597342590884,\n",
       "  0.04948356934673396,\n",
       "  -0.0368192989347208,\n",
       "  0.0066128325730812725,\n",
       "  -0.05402086630132191,\n",
       "  0.06544012906493839,\n",
       "  0.00942711268585168,\n",
       "  -0.14003847327412292,\n",
       "  0.027678123328473192,\n",
       "  -0.024907247810668574,\n",
       "  0.06029466012331888,\n",
       "  -0.016807239895609543,\n",
       "  -0.0875419342687434,\n",
       "  -0.04261577293113893,\n",
       "  0.11665006511772064,\n",
       "  0.10424347789356216,\n",
       "  0.09065668476775407,\n",
       "  -0.0712477460271564,\n",
       "  0.010048375374466706,\n",
       "  -0.035087724165988966,\n",
       "  -0.26974756602822175,\n",
       "  0.06364401282229601,\n",
       "  -0.0440539067606717,\n",
       "  0.032875744690894444,\n",
       "  -0.12675307621723958,\n",
       "  0.05949009368330627,\n",
       "  -0.035142339244565016,\n",
       "  -0.06629684179788417,\n",
       "  0.10547315866192342,\n",
       "  -0.09329974374435464,\n",
       "  -0.01413198842247681,\n",
       "  0.012264866018115559,\n",
       "  0.052136244650516694,\n",
       "  -0.12438108362305518,\n",
       "  0.09422845669063583,\n",
       "  0.024318834761198702,\n",
       "  -0.18860665956392872,\n",
       "  -0.028167241874781743,\n",
       "  -0.06055845386204567,\n",
       "  -0.012542797652843591,\n",
       "  0.04552125605660517,\n",
       "  -0.12273535036829943,\n",
       "  0.12492030875498858,\n",
       "  0.028351983965074982,\n",
       "  -0.11952271492079339,\n",
       "  -0.057786588021433924,\n",
       "  0.01842137043937127,\n",
       "  0.07758446316468161,\n",
       "  0.17778459464394572,\n",
       "  -0.08445734149200396,\n",
       "  -0.07665869782376389,\n",
       "  -0.061262283591699815,\n",
       "  0.10821497359691001,\n",
       "  -0.06079697069079727,\n",
       "  -0.029839746094171577,\n",
       "  0.11188236825010185,\n",
       "  -0.014025794936675703,\n",
       "  0.037293296018738355,\n",
       "  0.03914765625821272,\n",
       "  -0.05283471247849066,\n",
       "  -0.10290415313711848,\n",
       "  -0.06730850085206634,\n",
       "  -0.027857464350132557,\n",
       "  0.03973135534008262,\n",
       "  -0.020311469840145695,\n",
       "  -0.010761686714304334,\n",
       "  0.0005682237091903844,\n",
       "  0.18333041263830585,\n",
       "  0.05020543802966988,\n",
       "  0.014641119198898793,\n",
       "  -0.03685163802530108,\n",
       "  -0.026434765557483978,\n",
       "  0.03262837036513885,\n",
       "  -0.06272270618074816,\n",
       "  -0.03044923595868802,\n",
       "  -0.03855249731768479,\n",
       "  0.08164073704134985,\n",
       "  -0.1211015880589725,\n",
       "  0.13156307126422065,\n",
       "  0.009166898173689245,\n",
       "  0.06275334381075298,\n",
       "  -0.05704811132077338,\n",
       "  0.09680177347833328,\n",
       "  0.14154389552867944,\n",
       "  -0.05001758962771766,\n",
       "  -0.20960397895325725,\n",
       "  -0.13551735469666376,\n",
       "  -0.1280145419684481,\n",
       "  0.11860323505658679,\n",
       "  -0.049118733797084646,\n",
       "  0.13515158421349632,\n",
       "  0.07862071972877396,\n",
       "  -0.18663725717170848,\n",
       "  0.00882031419443424,\n",
       "  0.03322616529629942,\n",
       "  -0.09956245225996994,\n",
       "  0.02232274004274916,\n",
       "  -0.07986310767739116,\n",
       "  0.08770652491895717,\n",
       "  0.05949570171665659,\n",
       "  -0.10071625706500445,\n",
       "  0.11503377536927364,\n",
       "  0.13539128876038262,\n",
       "  0.07623197845334144,\n",
       "  0.009964564616568593,\n",
       "  -0.06909490739713033,\n",
       "  -0.10260165575863398,\n",
       "  0.030072505680997518,\n",
       "  -0.012282756390174576,\n",
       "  0.005910365460746741,\n",
       "  0.048478421887372426,\n",
       "  0.02820791825244586,\n",
       "  -0.13517636602153427,\n",
       "  -0.006632401079868198,\n",
       "  -0.027255415619576732,\n",
       "  -0.05772995973028955,\n",
       "  0.11863800548045338,\n",
       "  0.03402915829294161,\n",
       "  0.02235535436053372,\n",
       "  0.03567663902849404,\n",
       "  -0.019602554762065155,\n",
       "  0.102855203353515,\n",
       "  -0.20151590466411082,\n",
       "  0.039934477393958535,\n",
       "  0.09400363669062899,\n",
       "  0.11469328949504339,\n",
       "  -0.10652527503191832,\n",
       "  0.07441283665010923,\n",
       "  -0.0329971296479177,\n",
       "  -0.09092755765576055,\n",
       "  -0.02374389954259662,\n",
       "  -0.028033841300560653,\n",
       "  -0.02323900349383668,\n",
       "  0.06897644462050868,\n",
       "  0.06365356577278637,\n",
       "  -0.06818480290009962,\n",
       "  -0.09145223479357477,\n",
       "  0.11278342297109385,\n",
       "  -0.006432212754566915,\n",
       "  0.04429745271808121,\n",
       "  -0.013898266122671072,\n",
       "  0.13839248584195357,\n",
       "  0.062103698048428954,\n",
       "  -0.03378862915727817,\n",
       "  0.041800832680191864,\n",
       "  0.09719491731424143,\n",
       "  -0.0029781015811702893,\n",
       "  0.0838550514792844,\n",
       "  0.041908316279916945,\n",
       "  -0.033279983544996714,\n",
       "  -0.06202626148353708,\n",
       "  0.05118042080266603,\n",
       "  0.052605818576992064,\n",
       "  -0.08264539342326903,\n",
       "  -0.06371188153576923,\n",
       "  0.004337611873692238,\n",
       "  0.04631411027756831,\n",
       "  -0.026653359256415016,\n",
       "  0.03675092756708122,\n",
       "  -0.030983849491570632,\n",
       "  -0.0713243375863476,\n",
       "  0.0013428733964195702,\n",
       "  -0.10166178563154664,\n",
       "  -0.03984788855712338,\n",
       "  -0.08668377367130484,\n",
       "  0.05603103883740931,\n",
       "  -0.01748534998520071,\n",
       "  0.04506258736544387,\n",
       "  0.01606411814596408,\n",
       "  -0.024207999172988753,\n",
       "  -0.012812190618577237,\n",
       "  -0.062005930331201844,\n",
       "  -0.1059110148578383,\n",
       "  -0.030942684294537486,\n",
       "  0.14066771385273177,\n",
       "  0.05996050121820201,\n",
       "  -0.08469827654813718,\n",
       "  0.12240996097044496,\n",
       "  -0.12423736194648838,\n",
       "  -0.016051378003414454,\n",
       "  0.06487560295729052,\n",
       "  0.09338520304328218,\n",
       "  0.10972281804440209,\n",
       "  -0.07111111432075339,\n",
       "  -0.07611746440038625,\n",
       "  -0.04496160947954023,\n",
       "  -0.0735330885508743,\n",
       "  0.01828275854306231,\n",
       "  -0.048148769770832794,\n",
       "  0.09563634898291114,\n",
       "  -0.03974691570173653,\n",
       "  0.06496111699675693,\n",
       "  -0.06596569035018383,\n",
       "  -0.06871499478561352,\n",
       "  -0.012270806508169304,\n",
       "  0.16607377340087423,\n",
       "  -0.02432451220828366,\n",
       "  -0.05947393413328881,\n",
       "  0.027453517217369876,\n",
       "  -0.11782193882904281,\n",
       "  0.07968027845800105,\n",
       "  -0.12020980002101635,\n",
       "  0.01281105415904757,\n",
       "  0.05103682140598822,\n",
       "  0.039330224021662745,\n",
       "  -0.06670296405734308,\n",
       "  0.11050840257335905,\n",
       "  0.07495102125915233,\n",
       "  -0.007695167097480572,\n",
       "  -0.10118653992384351,\n",
       "  -0.0329868375125386,\n",
       "  -0.053944579118865174,\n",
       "  0.02117099010507754,\n",
       "  0.06105229251882666,\n",
       "  -0.030282752155005525,\n",
       "  0.2022854189611981,\n",
       "  0.1382314761997213,\n",
       "  -0.11377243462421939,\n",
       "  0.002568651314806142,\n",
       "  -0.0009077034517041808,\n",
       "  0.04182439159609849,\n",
       "  0.05945254301407715]]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preparing data\n",
    "\n",
    "# Create word vectors for tweets\n",
    "test['mean_embed'] = test['tweet'].map(word_vector)\n",
    "\n",
    "# Preprocess test data\n",
    "xtest = preprocess(test.mean_embed.copy().tolist())\n",
    "\n",
    "xtest[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models computed so far are the following.\n",
      " \n",
      "Classifier                                         | R2 Score            \n",
      "-----------------------------------------------------------------\n",
      "Linear Model                                       | 0.0528607913\n",
      "Logistic Regression                                | 0.5927200000\n",
      "Logistic Regression using cross-validation         | 0.5928000000\n",
      "SVM classifier                                     | 0.5940400000\n",
      "Neural Network                                     | 0.6162800000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recalling classifiers \n",
    "# stored in format : 'classifier name'=(classifier, R2 score) \n",
    "\n",
    "print(f\"Models computed so far are the following.\\n \")\n",
    "print(f\"{'Classifier':50s} | {'R2 Score':20s}\")\n",
    "print(f\"-----------------------------------------------------------------\")\n",
    "for k,v in classifiers.items() :\n",
    "    print(f\"{k:50s} | {v[1]:10.10f}\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "model = classifiers['Neural Network'][0]\n",
    "\n",
    "predictions = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating submission file\n",
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "            \n",
    "create_csv_submission(test.index, predictions, '../submission/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Here we detail previously computed results from `results/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Dimensions evolution\n",
    "Below we detail how different metrics perform when the dimension of the word embeddings computed by GloVe algorithm changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 dimensions\n",
      "\n",
      "Accuracy : 0.585\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.574874 |   0.671121 |   0.619280\n",
      "neg        |   0.599399 |   0.497868 |   0.543936\n",
      "\n",
      "50 dimensions\n",
      "\n",
      "Accuracy : 0.5896\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.575301 |   0.671701 |   0.619775\n",
      "neg        |   0.609460 |   0.508167 |   0.554223\n",
      "\n",
      "100 dimensions\n",
      "\n",
      "Accuracy : 0.60904\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.587911 |   0.726698 |   0.649979\n",
      "neg        |   0.643141 |   0.491608 |   0.557257\n",
      "\n",
      "250 dimensions\n",
      "\n",
      "Accuracy : 0.61628\n",
      "Label      | precision  | recall     | f1-score  \n",
      "pos        |   0.634955 |   0.542312 |   0.584988\n",
      "neg        |   0.602425 |   0.689859 |   0.643184\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "f1s = []\n",
    "\n",
    "\n",
    "for d in [20,50,100,250] :\n",
    "    print(f'\\n{d} dimensions\\n')\n",
    "    name = path+'metrics_'+str(local_t_size)+'_'+str(d)+'_baseline.npy'\n",
    "    r = np.load(name, allow_pickle=True)\n",
    "    r = r.item()\n",
    "    print(f\"Accuracy : {r['accuracy']}\")\n",
    "    print(f\"{'Label':10s} | {'precision':10s} | {'recall':10s} | {'f1-score':10s}\")\n",
    "    print(f\"{'pos':10s} | {r['pos']['precision']:10f} | {r['pos']['recall']:10f} | {r['pos']['f1-score']:10f}\")    \n",
    "    print(f\"{'neg':10s} | {r['neg']['precision']:10f} | {r['neg']['recall']:10f} | {r['neg']['f1-score']:10f}\")  \n",
    "    accs.append(r['accuracy'])\n",
    "    precs.append(r['pos']['precision'])\n",
    "    recs.append(r['pos']['recall'])\n",
    "    f1s.append(r['pos']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [20,50,100,250]\n",
    "plt.plot(x, accs)\n",
    "plt.plot(x, precs)\n",
    "plt.plot(x, recs)\n",
    "plt.plot(x, f1s)\n",
    "plt.legend(['accuracy','precision','recall','f1-score'])\n",
    "plt.xticks(ticks=np.arange(0,260,20))\n",
    "plt.vlines(190,ymin=0.53, ymax=0.725, color='black')\n",
    "name = '../plots/'+'metrics_'+str(local_t_size)+'_dims_baseline.png'\n",
    "plt.savefig(name, dpi=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the optimal number of dimensions seems to lie around **190** as the parameters are at their common highest at this point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
