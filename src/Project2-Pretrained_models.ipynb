{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained NNLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://www.tensorflow.org/hub/tutorials/tf2_text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Installations\n",
    "!pip install tensorflow_hub\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = '../data/'\n",
    "# Load training set\n",
    "\n",
    "pos = pd.read_table(data_path + \"train_pos_full.txt\", sep='.\\n', names=['tweet'], engine='python')\n",
    "pos['label']=1\n",
    "print(f\"Loaded POS data, correctly interpreted 1-tweet-per-line fashion : {pos.shape[0]==1_250_000}\")\n",
    "neg = pd.read_table(data_path + \"train_neg_full.txt\", sep='.\\n', names=['tweet'], engine='python')\n",
    "neg['label']=-1\n",
    "print(f\"Loaded NEG data, correctly interpreted 1-tweet-per-line fashion : {neg.shape[0]==1_250_000}\")\n",
    "print(f\"Data sizes : (POS) {pos.shape[0]} (NEG) {neg.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets = pos.merge(neg, how='outer')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tensor = tf.constant(tweets['tweet'].values)\n",
    "labels_tensor = tf.constant(tweets['label'].values)\n",
    "dataset = (tweets_tensor, labels_tensor)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=dataset\n",
    "train_examples, train_labels = tfds.as_numpy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the model\n",
    "#model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "#Largest model\n",
    "model = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[128], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_examples[:10000]\n",
    "partial_x_train = train_examples[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=500000,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "## Test the model\n",
    "\n",
    "# To format the testing data\n",
    "def extract_tweet(tweet):\n",
    "    return tweet.split(\",\", 1)[1]\n",
    "\n",
    "# Load the testing data\n",
    "test = pd.read_fwf(data_path +\"test_data.txt\", sep=\"\\n\", header=None)\n",
    "test.index = pd.RangeIndex(start=1, stop=10001, step=1) # Format asked by AI Crowd\n",
    "test = test[0].map(extract_tweet)\n",
    "test = pd.DataFrame(test)\n",
    "test.columns = ['tweet']\n",
    "test\n",
    "\n",
    "test_tensor=test['tweet'].values\n",
    "\n",
    "def log_odd_convert(x):\n",
    "    return -1 if x<0 else 1\n",
    "results = list(map(lambda x:log_odd_convert(x), model.predict(test_tensor)))\n",
    "test['label']=results\n",
    "\n",
    "## Export the result\n",
    "\n",
    "test = test.drop('tweet', axis=1)\n",
    "test.index.name='Id'\n",
    "test = test.rename(columns={'label':'Prediction'})\n",
    "test\n",
    "\n",
    "with open(data_path + \"submission.csv\", 'w') as f:\n",
    "    test.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr = {\n",
    "  # SMS abbreviations\n",
    "  'omg': 'oh my god',\n",
    "  'afk':'away from keyboard',\n",
    "  'bf':'boyfriend',\n",
    "  'bff':'best friend forever',\n",
    "  'lol' : 'laughing out loud',\n",
    "  'irl' : 'in my opinion',\n",
    "  'gf' :'girlfriend',\n",
    "  'idk' : \"i don't know\",\n",
    "  'fyi':'for your information',\n",
    "  'asap' : 'as soon as possible',\n",
    "  'yolo':'you live only once',\n",
    "  'smh':'shaking my head',\n",
    "  'btw' : 'by the way',\n",
    "  'otw':'on the way',\n",
    "  'msg':'message',\n",
    "  'ppl' : 'people',\n",
    "  'np' : 'no problem',\n",
    "  'imy':'i miss you',\n",
    "  'jk' : 'just kidding',\n",
    "  'fyi' : 'for your information',\n",
    "  'idc' : \"i don't care\",\n",
    "  'gg' : 'good game',\n",
    "  'thx' : 'thanks',\n",
    "  'lmao' : 'laughing my ass off',\n",
    "  'ily':'i love you',\n",
    "  'rofl' : 'rolling on floor laughing',\n",
    "  'stfu' : 'shut the fuck up',\n",
    "  'y' : 'you',\n",
    "  'yolo':'you only live once',\n",
    "  'wtf' : 'what the fuck',\n",
    "  'wth':'what the hell',\n",
    "  # smileys\n",
    "  ':|' : \"i'm indecisive\",\n",
    "  ':[' : \"i'm sad\",\n",
    "  ':@' : \"i'm angry\",\n",
    "  ':{' : \"i'm sad\",\n",
    "  'xd' : \"i'm laughing\",\n",
    "  ':/' : \"i'm skeptical\",\n",
    "  ':p' : \"i'm cheeky\",\n",
    "  ':d' : \"i'm smiling\",\n",
    "  ':$' : \"i'm embarrassed\",\n",
    "  \":')\" : \"i'm joyful\",\n",
    "  '=)' : \"i'm smiling\",\n",
    "  'd:' : \"i'm smiling\",\n",
    "  'xx' : 'two kisses',\n",
    "  'xxx' : 'hugs and kisses',\n",
    "  'xoxo' : 'hugs and kisses',\n",
    "  ':o' : \"i'm surprised\",\n",
    "  '<3' : 'love'\n",
    " }\n",
    "\n",
    "def formalize(tweet):\n",
    "    return ' '.join([abbr.get(x, x) for x in tweet.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Define Preprocessors\n",
    "\n",
    "def clean_HTML_tags(series) :\n",
    "    return series.str.replace('<\\/*[a-zA-Z]+>', '', regex=True)\n",
    "\n",
    "def pretrain_process(pos_,neg_):\n",
    "    pos = pos_.copy()\n",
    "    neg = neg_.copy()\n",
    "\n",
    "    # Drop duplicates\n",
    "    pos = pos.drop_duplicates()\n",
    "    neg = neg.drop_duplicates()\n",
    "    # Balance classes after having deleted duplicates\n",
    "    min_size = min(pos.shape[0], neg.shape[0])\n",
    "    pos = resample(pos, n_samples=min_size, replace=False)\n",
    "    neg = resample(neg, n_samples=min_size, replace=False)\n",
    "\n",
    "    pos['tweet'] = clean_HTML_tags(pos['tweet']).values\n",
    "    neg['tweet'] = clean_HTML_tags(neg['tweet']).values\n",
    "    pos['tweet'] = list(map(lambda x : formalize(x), pos['tweet'].values))\n",
    "    neg['tweet'] = list(map(lambda x : formalize(x), neg['tweet'].values))\n",
    "\n",
    "    return pos.merge(neg, how='outer')\n",
    "\n",
    "def preprocessing_test(test_):\n",
    "    test = test_.copy()\n",
    "    test['tweet'] = clean_HTML_tags(test['tweet']).values\n",
    "    test['tweet'] = list(map(lambda x : formalize(x), test['tweet'].values))\n",
    "    return test\n",
    "\n",
    "tweets = pretrain_process(pos, neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = resample(tweets, n_samples=10_000, replace=False)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(tweets_,\n",
    "                                                                   text_column='tweet',\n",
    "                                                                   label_columns=['label'],\n",
    "                                                                   preprocess_mode='bert', \n",
    "                                                                   lang='en',\n",
    "                                                                   verbose=True\n",
    "                                                                   )\n",
    "\n",
    "model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)\n",
    "learner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=12)\n",
    "\n",
    "# Uncomment to tune the learning rates\n",
    "# learner.lr_find(max_epochs=5, show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training report of the best prediction\n",
    "1st cycle : 8e-4, 10'000 samples, accuracy of  0.5542  \n",
    "2nd cycle : 2e-5, 30,000 samples, accuracy of  0.8147  \n",
    "3rd cycle : 2e-5, 30,000 samples, accuracy of  0.8512  \n",
    "4th cycle : 2e-5, 30,000 samples, accuracy of  0.8554    \n",
    "5th cycle : 1e-5, 30,000 samples, accuracy of  0.8588     \n",
    "6th cycle : 1e-5, 30,000 samples, accuracy of  0.8702    \n",
    "7th cycle : 1e-5, 30,000 samples, accuracy of  0.8713     \n",
    "8th cycle : 5e-6, 30,000 samples, accuracy of  0.8695    \n",
    "9th cycle : 5e-6, 30,000 samples, accuracy of  0.8707    \n",
    "10th cycle: 5e-7, 30,000 samples, accuracy of  0.8735    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "initial_n_samples = 10_000\n",
    "\n",
    "tweets_ = resample(tweets, n_samples=initial_n_samples, replace=False)\n",
    "train_n_samples = int(initial_n_samples*(1-0.01))\n",
    "train_df = tweets_[:train_n_samples]\n",
    "test_df = tweets_[train_n_samples:]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(tweets_,\n",
    "                                                                   text_column='tweet',\n",
    "                                                                   label_columns=['label'],\n",
    "                                                                   val_df=test_df,\n",
    "                                                                   preprocess_mode='bert', \n",
    "                                                                   lang='en',\n",
    "                                                                   verbose=True\n",
    "                                                                   )\n",
    "model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_cycle(model, tweets, n_samples, lr, batch_size, n_epochs=1):\n",
    "'''\n",
    "Fits the model with the given parameters using a cyclical learning rate\n",
    "'''\n",
    "    tweets_ = resample(tweets, n_samples=n_samples, replace=False)\n",
    "    \n",
    "    train_n_samples = int(n_samples*(1-0.01)) # nb of training samples\n",
    "    train_df = tweets_[:train_n_samples]\n",
    "    test_df = tweets_[train_n_samples:] # Validation set\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(tweets_,\n",
    "                                                                   text_column='tweet',\n",
    "                                                                   label_columns=['label'],\n",
    "                                                                   val_df=test_df,\n",
    "                                                                   preprocess_mode='bert', \n",
    "                                                                   lang='en',\n",
    "                                                                   verbose=True\n",
    "                                                                   )\n",
    "    learner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=batch_size)\n",
    "    learner.fit_onecycle(lr, n_epochs)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running this cell multiple times will train the model \n",
    "\n",
    "learner = run_one_cycle(model, tweets, 10, 2e-5, 6) #default lr is 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing data\n",
    "test = pd.read_fwf(data_path + \"test_data.txt\", sep=\"\\n\", header=None)\n",
    "# To format the testing data\n",
    "def extract_tweet(tweet):\n",
    "    return tweet.split(\",\", 1)[1]\n",
    "# Formatting for AI Crowd\n",
    "test.index = pd.RangeIndex(start=1, stop=10001, step=1) \n",
    "test = test[0].map(extract_tweet)\n",
    "test = pd.DataFrame(test)\n",
    "test.columns = ['tweet']\n",
    "test= preprocessing_test(test)\n",
    "test_values=test['tweet'].values\n",
    "# Make the predictions\n",
    "test['label']=p.predict(test_values)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export the result\n",
    "test = test.drop('tweet', axis=1)\n",
    "test.index.name='Id'\n",
    "test = test.rename(columns={'label':'Prediction'})\n",
    "\n",
    "with open(data_path + \"submission.csv\", 'w') as f:\n",
    "    test.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Running this cell will reproduce the submission file with the best predictions\n",
    "\n",
    "\n",
    "Note : batch_size may need to be decreased according to the setup\n",
    "Computation may take many hours\n",
    "'''\n",
    "\n",
    "# Imports\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "data_path = '../data/'\n",
    "\n",
    "# Load training set\n",
    "pos = pd.read_table(data_path + \"train_pos_full.txt\", sep='.\\n', names=['tweet'], engine='python')\n",
    "pos['label']=1\n",
    "print(f\"Loaded POS data, correctly interpreted 1-tweet-per-line fashion : {pos.shape[0]==1_250_000}\")\n",
    "neg = pd.read_table(data_path + \"train_neg_full.txt\", sep='.\\n', names=['tweet'], engine='python')\n",
    "neg['label']=-1\n",
    "tweets = pos.merge(neg, how='outer')\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "abbr = {\n",
    "  # SMS abbreviations\n",
    "  'omg': 'oh my god',\n",
    "  'afk':'away from keyboard',\n",
    "  'bf':'boyfriend',\n",
    "  'bff':'best friend forever',\n",
    "  'lol' : 'laughing out loud',\n",
    "  'irl' : 'in my opinion',\n",
    "  'gf' :'girlfriend',\n",
    "  'idk' : \"i don't know\",\n",
    "  'fyi':'for your information',\n",
    "  'asap' : 'as soon as possible',\n",
    "  'yolo':'you live only once',\n",
    "  'smh':'shaking my head',\n",
    "  'btw' : 'by the way',\n",
    "  'otw':'on the way',\n",
    "  'msg':'message',\n",
    "  'ppl' : 'people',\n",
    "  'np' : 'no problem',\n",
    "  'imy':'i miss you',\n",
    "  'jk' : 'just kidding',\n",
    "  'fyi' : 'for your information',\n",
    "  'idc' : \"i don't care\",\n",
    "  'gg' : 'good game',\n",
    "  'thx' : 'thanks',\n",
    "  'lmao' : 'laughing my ass off',\n",
    "  'ily':'i love you',\n",
    "  'rofl' : 'rolling on floor laughing',\n",
    "  'stfu' : 'shut the fuck up',\n",
    "  'y' : 'you',\n",
    "  'yolo':'you only live once',\n",
    "  'wtf' : 'what the fuck',\n",
    "  'wth':'what the hell',\n",
    "  # smileys\n",
    "  ':|' : \"i'm indecisive\",\n",
    "  ':[' : \"i'm sad\",\n",
    "  ':@' : \"i'm angry\",\n",
    "  ':{' : \"i'm sad\",\n",
    "  'xd' : \"i'm laughing\",\n",
    "  ':/' : \"i'm skeptical\",\n",
    "  ':p' : \"i'm cheeky\",\n",
    "  ':d' : \"i'm smiling\",\n",
    "  ':$' : \"i'm embarrassed\",\n",
    "  \":')\" : \"i'm joyful\",\n",
    "  '=)' : \"i'm smiling\",\n",
    "  'd:' : \"i'm smiling\",\n",
    "  'xx' : 'two kisses',\n",
    "  'xxx' : 'hugs and kisses',\n",
    "  'xoxo' : 'hugs and kisses',\n",
    "  ':o' : \"i'm surprised\",\n",
    "  '<3' : 'love'\n",
    " }\n",
    "\n",
    "def formalize(tweet):\n",
    "    return ' '.join([abbr.get(x, x) for x in tweet.split()])\n",
    "\n",
    "# Define Preprocessors\n",
    "\n",
    "def clean_HTML_tags(series) :\n",
    "    return series.str.replace('<\\/*[a-zA-Z]+>', '', regex=True)\n",
    "\n",
    "def pretrain_process(pos_,neg_):\n",
    "    pos = pos_.copy()\n",
    "    neg = neg_.copy()\n",
    "\n",
    "    # Drop duplicates\n",
    "    pos = pos.drop_duplicates()\n",
    "    neg = neg.drop_duplicates()\n",
    "    # Balance classes after having deleted duplicates\n",
    "    min_size = min(pos.shape[0], neg.shape[0])\n",
    "    pos = resample(pos, n_samples=min_size, replace=False)\n",
    "    neg = resample(neg, n_samples=min_size, replace=False)\n",
    "\n",
    "    pos['tweet'] = clean_HTML_tags(pos['tweet']).values\n",
    "    neg['tweet'] = clean_HTML_tags(neg['tweet']).values\n",
    "    pos['tweet'] = list(map(lambda x : formalize(x), pos['tweet'].values))\n",
    "    neg['tweet'] = list(map(lambda x : formalize(x), neg['tweet'].values))\n",
    "\n",
    "    return pos.merge(neg, how='outer')\n",
    "\n",
    "def preprocessing_test(test_):\n",
    "    test = test_.copy()\n",
    "    test['tweet'] = clean_HTML_tags(test['tweet']).values\n",
    "    test['tweet'] = list(map(lambda x : formalize(x), test['tweet'].values))\n",
    "    return test\n",
    "\n",
    "def run_one_cycle(model, tweets, n_samples, lr, batch_size, n_epochs=1):\n",
    "'''\n",
    "Fits the model with the given parameters using a cyclical learning rate\n",
    "'''\n",
    "    tweets_ = resample(tweets, n_samples=n_samples, replace=False)\n",
    "    \n",
    "    train_n_samples = int(n_samples*(1-0.01)) # nb of training samples\n",
    "    train_df = tweets_[:train_n_samples]\n",
    "    test_df = tweets_[train_n_samples:] # Validation set\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(tweets_,\n",
    "                                                                   text_column='tweet',\n",
    "                                                                   label_columns=['label'],\n",
    "                                                                   val_df=test_df,\n",
    "                                                                   preprocess_mode='bert', \n",
    "                                                                   lang='en',\n",
    "                                                                   verbose=True\n",
    "                                                                   )\n",
    "    learner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=batch_size)\n",
    "    learner.fit_onecycle(lr, n_epochs)\n",
    "    return learner\n",
    "\n",
    "tweets = pretrain_process(pos, neg)\n",
    "\n",
    "# Parameters for the best accuracy\n",
    "learning_rates=[8e-4,2e-5,2e-5,2e-5,1e-5,1e-5,1e-5, 5e-6, 5e-6, 5e-7]\n",
    "nb_samples=[10_000,30_000,30_000,30_000,30_000,30_000,30_000,30_000,30_000,30_000]\n",
    "\n",
    "\n",
    "# Initialize the model \n",
    "initial_n_samples = 10_000\n",
    "batch_size=6\n",
    "\n",
    "tweets_ = resample(tweets, n_samples=initial_n_samples, replace=False)\n",
    "train_n_samples = int(initial_n_samples*(1-0.01))\n",
    "train_df = tweets_[:train_n_samples]\n",
    "test_df = tweets_[train_n_samples:]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(tweets_,\n",
    "                                                                   text_column='tweet',\n",
    "                                                                   label_columns=['label'],\n",
    "                                                                   val_df=test_df,\n",
    "                                                                   preprocess_mode='bert', \n",
    "                                                                   lang='en',\n",
    "                                                                   verbose=True\n",
    "                                                                   )\n",
    "model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)\n",
    "\n",
    "for i in range(0,10):\n",
    "    learner = run_one_cycle(model, tweets, nb_samples[i], learning_rates[i], batch_size) \n",
    "    \n",
    "# Load the testing data\n",
    "test = pd.read_fwf(data_path + \"test_data.txt\", sep=\"\\n\", header=None)\n",
    "\n",
    "# To format the testing data\n",
    "def extract_tweet(tweet):\n",
    "    return tweet.split(\",\", 1)[1]\n",
    "\n",
    "# Formatting for AI Crowd\n",
    "test.index = pd.RangeIndex(start=1, stop=10001, step=1) \n",
    "test = test[0].map(extract_tweet)\n",
    "test = pd.DataFrame(test)\n",
    "test.columns = ['tweet']\n",
    "test= preprocessing_test(test)\n",
    "test_values=test['tweet'].values\n",
    "# Make the predictions\n",
    "test['label']=p.predict(test_values)\n",
    "## Export the result\n",
    "test = test.drop('tweet', axis=1)\n",
    "test.index.name='Id'\n",
    "test = test.rename(columns={'label':'Prediction'})\n",
    "\n",
    "with open(data_path + \"submission.csv\", 'w') as f:\n",
    "    test.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada2]",
   "language": "python",
   "name": "conda-env-ada2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
